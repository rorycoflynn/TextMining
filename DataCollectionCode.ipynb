{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## IMPORTS\n",
    "import pandas as pd \n",
    "import requests\n",
    "import praw\n",
    "import time\n",
    "import pprint\n",
    "import logging\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from prawcore.exceptions import RequestException, ResponseException, ServerError\n",
    "from newspaper import Article\n",
    "from newsapi import NewsApiClient\n",
    "from bs4 import BeautifulSoup\n",
    "from requests.exceptions import RequestException \n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import glob \n",
    "\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import nltk\n",
    "#nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## INIT APIs \n",
    "\n",
    "newsKey = '992b1247b5b448e3935baac5af4a841c' #coryFlynn\n",
    "newsAPI = NewsApiClient(api_key=newsKey)\n",
    "\n",
    "#searchQuery=''\"AI art\" OR \"AI Image\" OR \"AI Artists\" OR \"AI-generated art\" OR \"AI generated images\" or \"AI generation\" OR \"generative AI art\" OR \"algorithmic art\" -OLED -Pendlebury -Pypi.org -The-next-web -byteDance -samsung',\n",
    "\n",
    "#searchQuery='\"art\" OR \"painting\" OR \"drawings\" OR \"illustration\" OR \"illustrator\" \"graphic design\" OR \"animation\" OR \"fine arts\" OR \"mural\" OR \"creative work\" OR \"art exhibition\" OR \"museum\" OR \"gallery\" -\"AI art\" -\"AI-generated\" -\"AI Images\" -\"machine learning art\" -OLED -The-next-web -byteDance -samsung',\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,  #see all logs\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    force=True  # rest config\n",
    ")\n",
    "\n",
    "#INIT reddit PRAW API \n",
    "reddit = praw.Reddit(\n",
    "    client_id='JBh-NQdNIpzU5_QWija3LQ', \n",
    "    client_secret='AjBxNAXhL4hPvdWf6xYUpA4sg9ATLg', \n",
    "    user_agent='script:top_posts_scraper:v1.0 (by u/bingabanggg)',\n",
    ")\n",
    "\n",
    "# subredditName = 'aiArt'  #name of subreddits to scrape\n",
    "# subreddit = reddit.subreddit(subredditName)\n",
    "\n",
    "#          - https://www.reddit.com/r/aiArt/ (PRO AI ART) \n",
    "#           - https://www.reddit.com/r/Art/ (ANTI AI ART) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##HELPER FUNCTIONS\n",
    "\n",
    "def cleanText(text):\n",
    "    \"\"\"\n",
    "    run this on any text to remove punctuation, whitespace, and nonalphabetic characters\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    \n",
    "    #remove cruft and punctuation, extra spaces\n",
    "    text = re.sub(r'[,.;@#?!&$\\-\\']+', ' ', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\\"', ' ', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(' +', ' ', text, flags=re.IGNORECASE)\n",
    "\n",
    "    # remove nonalphabetical (keep spaces)\n",
    "    text = re.sub(r'[^a-zA-Z ]', ' ', text, flags=re.VERBOSE)\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "\n",
    "def getDateFromIso(isoDate):\n",
    "    \"\"\"\n",
    "    given an ISO will return date \n",
    "    \"\"\"\n",
    "    if not isoDate:\n",
    "        return \"\"\n",
    "    return isoDate.split(\"T\")[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def bs4ArticleScrape(url, newsApiExcerpt=None):\n",
    "#     \"\"\"\n",
    "#     given the article desc from newsAPI (which is cut around200 chars because of free api tier)\n",
    "#     this funct checks if the content scraped from bs4 in a <p> section matches the init part of the excerpt \n",
    "#     (so as to confirm it scraped the right content) and then returns the page content as a str. \n",
    "#     \"\"\"\n",
    "#     fullContent = \"\" #empty str to save the page content to \n",
    "\n",
    "#     #using try blcok so that errors wont break the system \n",
    "#     try:\n",
    "#         response = requests.get(url, timeout=10)            #send get request\n",
    "#         time.sleep(1)                                       # time delay to be nice to the server\n",
    "#         soup = BeautifulSoup(response.text, 'html.parser')  # take raw page content and use html parser\n",
    "#         paragraphs = soup.find_all('p')                     #grab all text in paragraphs, returns as list of <p>\n",
    "#         fullContent = ' '.join([p.get_text() for p in paragraphs]).strip()  #grabs only the text from lists of p\n",
    "        \n",
    "#         #compare scraped para to the article desc from newsAPI to see if we grabbed the 'article' \n",
    "#         if newsApiExcerpt:                                  #check if there is a desc that exists\n",
    "#             cleanedExcerpt = cleanText(newsApiExcerpt)      #use clean text function to clean both texts\n",
    "#             cleanedFull = cleanText(fullContent)            \n",
    "#             if cleanedExcerpt and cleanedExcerpt not in cleanedFull: #check 1) if clean text is not empty and 2) that cleaned exerpt not in cleanfull:\n",
    "#                 print(\"Warning: The scraped content does not appear to include the expected NewsAPI excerpt.\")\n",
    "#                 #if article desc and scraped content dont match, printed response will let me know \n",
    "#     except Exception as e:\n",
    "#         print(f\"Error fetching full article from {url}: {e}\")\n",
    "#     return fullContent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# test URL\n",
    "testUrl = \"https://www.androidpolice.com/ai-art-in-project-zomboids-update-sparks-community-outrage/\"\n",
    "\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/97.0.4692.71 Safari/537.36\",\n",
    "    \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "}\n",
    "\n",
    "try:\n",
    "    response = requests.get(testUrl, headers=headers, timeout=10)\n",
    "    response.raise_for_status()  # yell when theres a bad responses\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Extract main article content (i skimmed, and sites use <article> or <p>)\n",
    "    paragraphs = soup.find_all(\"p\")\n",
    "    full_text = \"\\n\".join([p.get_text() for p in paragraphs])\n",
    "\n",
    "    print(\"\\n--- Extracted Full Article ---\\n\")\n",
    "    print(full_text[:1000])  # Print first 1000 characters\n",
    "\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"Error fetching article: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##fetch articles and get them as JSON\n",
    "\n",
    "def fetchNewsArticles(searchQuery, pageSize=20, sortBy=\"relevancy\", testMode=False):\n",
    "    \"\"\"\n",
    "    request to newsAPI and save as JSON file with all data. \n",
    "    testmode limits the call to one page \n",
    "    \"\"\"\n",
    "\n",
    "    allArticles = []\n",
    "    maxPages = 1 if testMode else 5  # Test mode limits to 1 page for quick testing\n",
    "    fromDate = (datetime.datetime.now() - datetime.timedelta(days=30)).strftime('%Y-%m-%d')  # Get max allowed date\n",
    "\n",
    "    for page in range(1, maxPages + 1):  #get max amnt of pages (API thing)\n",
    "        try:\n",
    "            #tell what page we're on, make api request\n",
    "            print(f\"Fetching page {page}...\")\n",
    "\n",
    "            response = newsAPI.get_everything(\n",
    "                q=searchQuery,\n",
    "                language=\"en\",\n",
    "                sort_by=sortBy,\n",
    "                page=page,\n",
    "                page_size=pageSize,\n",
    "                from_param=fromDate,\n",
    "            )\n",
    "        \n",
    "            if \"articles\" in response: #return count of articles retreived per page\n",
    "                articlesCount = len(response[\"articles\"])\n",
    "                print(f\"Page {page} returned {articlesCount} articles.\")\n",
    "\n",
    "                # Extract relevant fields\n",
    "                for article in response[\"articles\"]:\n",
    "                        #each of these has a value for if it doesnt exist - this helps flagging articles for data cleaning review later \n",
    "                    structured_article = {\n",
    "                        \"source\": article[\"source\"][\"name\"] if article.get(\"source\") else \"Unknown\", #conditional: if source exists, get name, else \"unknown\"\n",
    "                        \"author\": article.get(\"author\", \"Unknown\"),\n",
    "                        \"title\": article.get(\"title\", \"No Title\"),\n",
    "                        \"description\": article.get(\"description\", \"No Description\"),\n",
    "                        \"content\": article.get(\"content\", \"No Content Available\"),\n",
    "                        \"url\": article.get(\"url\", \"No URL\"),\n",
    "                        \"publishedAt\": article.get(\"publishedAt\", \"No Date\"),\n",
    "                    }\n",
    "                    allArticles.append(structured_article)\n",
    "\n",
    "            else:\n",
    "                print(f\"Page {page} returned no articles.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching page {page}: {e}\") \n",
    "            break\n",
    "\n",
    "        # Ratelimit safety\n",
    "        time.sleep(2)\n",
    "\n",
    "    #timestamp of file (for file management craziness)\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    jsonFileName = f\"newsAPI_{timestamp}.json\"\n",
    "\n",
    "    #save and return as JSON\n",
    "    with open(jsonFileName, \"w\", encoding=\"utf-8\") as jsonFile:\n",
    "        json.dump(allArticles, jsonFile, indent=4, ensure_ascii=False)\n",
    "\n",
    "    print(f\"Articles saved to {jsonFileName}\")\n",
    "\n",
    "    return jsonFileName\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #EXPERIMENTING WITH QUERIES IN FUNCTION 'testing mode' \n",
    "# #LETS ME SEE WHAT TERMS WORK THE BEST TO GET MOST RELEVANT ARTICLES \n",
    "\n",
    "# #AI ART SEARCH 1 - DIDNT WORK, RETURNED ARTICLES RELATING TO 'AI' ONLY. \n",
    "# searchQuery = \"AI art\"\n",
    "# language = \"en\"\n",
    "# pageSize = 100\n",
    "# sortBy = \"relevancy\"\n",
    "# fromDate = \"2024-12-27\"\n",
    "# language = 'en'\n",
    "\n",
    "\n",
    "# #ATTEMPT 2 \n",
    "# #WORKS - THERES SOME PRODUCTS THAT OFFER AI GEN ART \n",
    "# #AND SO PRODUCT ARTICLES ARE FILTERED IN.. THEYRE STILL RELEVANT THO. \n",
    "# searchQuery = '\"AI art\" OR \"AI-generated art\" OR \"artificial intelligence art\" OR \"AI artwork\"'\n",
    "# language = \"en\"\n",
    "# pageSize = 100\n",
    "# sortBy = \"relevancy\"\n",
    "# fromDate = \"2024-12-27\"\n",
    "\n",
    "# searchQuery = (\n",
    "#     '\"AI art\" OR \"AI-generated art\" OR \"artificial intelligence art\" OR \"AI artwork\" '\n",
    "#     'OR \"generative art\" OR \"machine learning art\" OR \"AI creativity\" '\n",
    "#     'OR \"AI artist\" OR \"AI-created art\" OR \"neural network art\" OR \"algorithmic art\"'\n",
    "# )\n",
    "\n",
    "\n",
    "# searchQuery = (\n",
    "#     '\"Midjourney\" OR \"Stable Diffusion\" OR \"DALL-E\" OR \"DALL-E 2\" OR \"RunwayML\" OR \"Leonardo AI\" '\n",
    "#     'OR \"Deep Dream\" OR \"DeepDream\" OR \"Artbreeder\" OR \"Dream by Wombo\" OR \"Wombo Dream\" '\n",
    "#     'OR \"Fotor AI\" OR \"NightCafe AI\" OR \"StarryAI\" OR \"Pixray\" OR \"DeepAI Image Generator\" '\n",
    "#     'OR \"Dreamlike Art\" OR \"DeepArt\" OR \"CF Spark\" OR \"Playground AI\" '\n",
    "#     'OR \"Deep Dream Generator\" OR \"RunDiffusion\" OR \"Mage AI\" OR \"Imagine AI\" '\n",
    "#     'OR \"InstantID\" OR \"GauGAN\" OR \"Synthesys AI\" OR \"Unreal Diffusion\"'\n",
    "# )\n",
    "\n",
    "'''\n",
    "\n",
    "\"art\" OR \"painting\" OR \"drawings\" OR \"illustration\" OR \n",
    "\"illustrator\" \"graphic design\" OR \"animation\" OR \"fine arts\" OR \"mural\" OR \n",
    "\"creative work\" OR \"art exhibition\" OR \"museum\" OR \"gallery\" -\"AI art\" \n",
    "-\"AI-generated\" -\"AI Images\" -\"machine learning art\" -OLED -The-next-web -byteDance -samsung\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#ATTEMPT 2 \n",
    "#WORKS - THERES SOME PRODUCTS THAT OFFER AI GEN ART \n",
    "#AND SO PRODUCT ARTICLES ARE FILTERED IN.. THEYRE STILL RELEVANT THO. \n",
    "searchQuery = '\"AI art\" OR \"AI-generated art\" OR \"artificial intelligence art\" OR \"AI artwork\"'\n",
    "language = \"en\"\n",
    "pageSize = 100\n",
    "sortBy = \"relevancy\"\n",
    "fromDate = \"2024-12-27\"\n",
    "\n",
    "#this runs the function in testmode to experiment without making rate limit an issue\n",
    "testArticles = fetchNewsArticles(\n",
    "    searchQuery=searchQuery,\n",
    "    pageSize=10,  # Smaller page size for testing\n",
    "    testMode=True  # Enable test mode\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##GET ARTICLES FOR ART \n",
    "###searchQuery='\"art\" OR \"painting\" OR \"drawings\" OR \"illustration\" OR \"illustrator\" \"graphic design\" OR \"animation\" OR \"fine arts\" OR \"mural\" OR \"creative work\" OR \"art exhibition\" OR \"museum\" OR \"gallery\" -\"AI art\" -\"AI-generated\" -\"AI Images\" -\"machine learning art\" -\"OLED\" -\"The-next-web\" -\"byteDance\" -\"samsung\"'\n",
    "\n",
    "\n",
    "\n",
    "#fetchNewsArticles('\"art\" OR \"painting\" OR \"drawings\" OR \"illustration\" \n",
    "#                   OR \"illustrator\" OR \"graphic design\" OR \"animation\" OR \"fine arts\" \n",
    "#                   OR \"mural\" OR \"creative work\" OR \"art exhibition\" OR \"museum\" OR \"gallery\" \n",
    "#                   -\"AI art\" -\"AI-generated\" -\"AI Images\" -\"machine learning art\" -\"OLED\" -\"The-next-web\" -\"byteDance\" -\"samsung\"')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadArticlesFromJSON(jsonFile):\n",
    "    \"\"\"\n",
    "    Reads output JSON from fetchNewsArticles articles and converts it to a df\n",
    "    \"\"\"\n",
    "    with open(jsonFile, \"r\", encoding=\"utf-8\") as file:\n",
    "        articles = json.load(file)\n",
    "\n",
    "    df = pd.DataFrame(articles)\n",
    "    \n",
    "    # make sure all columns are as they should be (otherwise add them with None values to be flagged later)\n",
    "    expectedColumns = [\"source\", \"author\", \"title\", \"description\", \"content\", \"url\", \"publishedAt\"]\n",
    "    for col in expectedColumns:\n",
    "        if col not in df:\n",
    "            df[col] = None\n",
    "    \n",
    "        if col not in df:\n",
    "            df[col] = None\n",
    "            \n",
    "    # lambda to each element in source column of DF - checks if element is dict with key 'name' - if yes, return name if not, return unknown. \n",
    "    # if isinstance(source, dict) and \"name\" in source:\n",
    "    #     return source[\"name\"]\n",
    "    # else:\n",
    "    #     return \"Unknown\"\n",
    "    df[\"source\"] = df[\"source\"].apply(lambda x: x[\"name\"] if isinstance(x, dict) and \"name\" in x else \"Unknown\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = loadArticlesFromJSON('newsAPI_2025-02-12_21-20-39.json')\n",
    "articles.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set keywords from the original query, these will be used to evaluate if the content scraped is truly relevant.\n",
    "###searchQuery='\"art\" OR \"painting\" OR \"drawings\" OR \"illustration\" OR \"illustrator\" \"graphic design\" OR \"animation\" OR \"fine arts\" OR \"mural\" OR \"creative work\" OR \"art exhibition\" OR \"museum\" OR \"gallery\" -\"AI art\" -\"AI-generated\" -\"AI Images\" -\"machine learning art\" -\"OLED\" -\"The-next-web\" -\"byteDance\" -\"samsung\"'\n",
    "Keywords = [\n",
    "    \"art\", \"painting\", \"drawings\", \"illustration\", \"illustrator\",\n",
    "    \"graphic design\", \"animation\", \"fine arts\", \"mural\", \"creative work\",\n",
    "    \"art exhibition\", \"museum\", \"gallery\"\n",
    "]\n",
    "\n",
    "### apply this to df to check if keywords appear in the text(exact matches only)\n",
    "def checkExactKeyword(text):\n",
    "    \"\"\"\n",
    "    Check if any Art-related keyword appears as an exact phrase in the text.\n",
    "    \"\"\"\n",
    "    if isinstance(text, str):\n",
    "        return any(f\" {kw} \" in f\" {text} \" for kw in Keywords)\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "filePath = \"ALL DATA/ART_articles.csv\"\n",
    "df = pd.read_csv(filePath)\n",
    "\n",
    "# Select two articles for comparison (adjust the indices if needed)\n",
    "article_1 = df.iloc[0]  # First article\n",
    "article_2 = df.iloc[1]  # Second article\n",
    "\n",
    "# Print a comparison of the fields\n",
    "print(\"Comparison of Article Fields:\\n\")\n",
    "\n",
    "print(f\"Title 1: {article_1['title']}\")\n",
    "print(f\"**Content** 1:\\n{article_1['content']}\\n\")\n",
    "print(f\"**FullArticleContent** 1:\\n{article_1['FullArticleContent']}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Title 2: {article_2['title']}\\n\")\n",
    "print(f\"**Content**:\\n{article_2['content']}\\n\")\n",
    "print(f\"**FullArticleContent** 2:\\n{article_2['FullArticleContent']}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###this checks for keywords and will ask approval for articles that pop up with not enough//missing content \n",
    "def filterAndProcessArticles(df):\n",
    "    \"\"\"\n",
    "    goes article by article to check for keywords actually IN the article content/body. \n",
    "    then flags for bad-data and has you check them one by one (usually only a dozen will need checking.)\n",
    "    \"\"\"\n",
    "    #drop any rows missing content \n",
    "    df.dropna(subset=['title', 'description', 'content', 'url'], inplace=True)\n",
    "\n",
    "    #keyword checks on all text columns\n",
    "    df[\"RelevantTitle\"] = df[\"title\"].apply(checkExactKeyword)\n",
    "    df[\"RelevantContent\"] = df[\"content\"].apply(checkExactKeyword)\n",
    "    df[\"RelevantDescription\"] = df[\"description\"].apply(checkExactKeyword)\n",
    "\n",
    "    #articles that dont match any keywords are flagged (new columns)\n",
    "    df[\"KeywordMatch\"] = df[[\"RelevantTitle\", \"RelevantContent\", \"RelevantDescription\"]].any(axis=1)\n",
    "\n",
    "    #all articles from keyword process that got flagged are grabbed for manual review\n",
    "    #all articles that have less than 50 chars are also flagged for review\n",
    "    df[\"ContentLength\"] = df[\"content\"].str.len().fillna(0) + df[\"description\"].str.len().fillna(0)\n",
    "    df[\"NeedsManualReview\"] = (df[\"ContentLength\"] < 50) | (~df[\"KeywordMatch\"])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###  fetch the full article content \n",
    "def fetchFullArticle(url):\n",
    "    \"\"\"\n",
    "    given url will use newspaper3k first then bs4 to try and get full article text. \n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        article = Article(url)\n",
    "        article.download()\n",
    "        article.parse()\n",
    "        \n",
    "        # check if the len grabbed is significant len\n",
    "        if len(article.text) > 100:\n",
    "            return article.text.strip()\n",
    "        \n",
    "        #if newspaper3k falls thru then use bs4\n",
    "        else:\n",
    "            print(f\"Newspaper3k extracted too little content for {url}. Trying BS4 fallback...\")\n",
    "    except Exception as e:\n",
    "        print(f\"Newspaper3k failed for {url}: {e}. Trying BS4 fallback...\")\n",
    "\n",
    "    # using bs4 instead \n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "\n",
    "        #skip if blocked \n",
    "        if response.status_code in [403, 429] or \"consent\" in response.url:\n",
    "            print(f\" Skipping (Blocked/Consent Page): {url}\")\n",
    "            return None\n",
    "\n",
    "        response.raise_for_status()\n",
    "\n",
    "        # find p tags and extract text. \n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        paragraphs = soup.find_all(\"p\")\n",
    "        fullText = \"\\n\".join([p.get_text() for p in paragraphs]).strip()\n",
    "\n",
    "        return fullText if len(fullText) > 100 else None  # minimum length for results so that bad articles automatically get cut\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"BS4 also failed for {url}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### clean save to DF\n",
    "def saveProcessedData(df):\n",
    "    approvedRows = df[df[\"NeedsManualReview\"] == False]\n",
    "    rejectedRows = df[df[\"NeedsManualReview\"] == True]\n",
    "\n",
    "    approvedRows.to_csv(cleanFileName, index=False)\n",
    "    rejectedRows.to_csv(rejectedFileName, index=False)\n",
    "\n",
    "    print(f\"Clean articles saved to {cleanFileName}\")\n",
    "    print(f\"Rejected articles saved to {rejectedFileName}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#master function runs all on given json file. \n",
    "def processAllArticles(jsonFile):\n",
    "    \"\"\"\n",
    "    Master function that loads, processes, and saves articles from a NewsAPI JSON file.\n",
    "    Ensures every article gets full-length content.\n",
    "    \"\"\"\n",
    "    print(\"Loading JSON data...\")\n",
    "    df = loadArticlesFromJSON(jsonFile)\n",
    "    \n",
    "    print(\"Filtering and processing articles...\")\n",
    "    df = filterAndProcessArticles(df)\n",
    "\n",
    "    print(\"Fetch full article content for given articles\")\n",
    "    df[\"FullArticleContent\"] = df[\"url\"].apply(fetchFullArticle) \n",
    "\n",
    "    print(\"saving processed data...\")\n",
    "    saveProcessedData(df)\n",
    "\n",
    "    print(\"file complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleanFileName=\"clean_art_articles.csv\"\n",
    "# rejectedFileName=\"rejected_art_articles.csv\"\n",
    "\n",
    "processAllArticles(\"newsAPI_2025-02-16_21-38-46.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## EXPERIMENTING WITH REDDIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subreddit_name = 'aiArt'  #name of subreddits to scrape\n",
    "# subreddit = reddit.subreddit(subreddit_name)\n",
    "\n",
    "#          - https://www.reddit.com/r/aiArt/ (PRO AI ART) \n",
    "#           - https://www.reddit.com/r/Art/ (ANTI AI ART) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isBotComment(commentText):\n",
    "    botPhrases = [\"i am a bot\", \"automated\", \"bot\", \"auto-mod\"]\n",
    "    return any(phrase in commentText for phrase in botPhrases)\n",
    "\n",
    "def handleRateLimit(waitSeconds=60):\n",
    "    logging.warning(f\"Rate limit reached. Waiting for {waitSeconds} seconds...\")\n",
    "    time.sleep(waitSeconds)\n",
    "\n",
    "\n",
    "#this will let me immediately save posts//incrimentally as I scrape reddit.\n",
    "def savePost(postData, filePath):\n",
    "    \"\"\"append to jsonl\"\"\"\n",
    "    try:\n",
    "        with open(filePath, 'a', encoding='utf-8') as f:\n",
    "            f.write(json.dumps(postData, ensure_ascii=False) + '\\n')\n",
    "    except IOError as e:\n",
    "        logging.error(f\"Failed to write to file {filePath}: {e}\")\n",
    "\n",
    "#COUNT LINES IN JSONL FILE\n",
    "def countJsonlLines(filePath):\n",
    "    \"\"\"returns the number of lines in the json file if it exists (filename issues lol)\"\"\"\n",
    "    if os.path.exists(filePath):\n",
    "        with open(filePath, 'r', encoding='utf-8') as f:\n",
    "            return sum(1 for _ in f)\n",
    "    return 0\n",
    "\n",
    "def loadExistingData(filePath):\n",
    "    \"\"\"makes sure json file exists before loading\"\"\"\n",
    "    if not os.path.exists(filePath):\n",
    "        return []\n",
    "    with open(filePath, 'r', encoding='utf-8') as f:\n",
    "        return [json.loads(line) for line in f]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##THIS IS ONLY FOR AI RELATED FILES\n",
    "#TO PROCESS THEM FOR MY USE NOW \n",
    "\n",
    "'''\n",
    "bc news api has the 30 day limit I wanted ot make the most use out of even the super\n",
    " messy early data files I had retrieved, especially for the URLs since I plan to use Bs4 to get the full articles anyways. '''\n",
    "\n",
    "def scrapeSubreddit(subredditName):\n",
    "    \"\"\"\n",
    "    goes through posts and sorts by ai related and nonAI related posts in a given subreddit. \n",
    "    grabs from hot , controverisal, and top and stops when it has 150 total posts related to AI \n",
    "    \"\"\"\n",
    "    logging.info(f\"Starting scrape for subreddit: {subredditName}\")\n",
    "\n",
    "    #create two CSV files, named for the source subreddit, where posts will be filtered.\n",
    "    #one file contains all posts with keywords relating to AI in them\n",
    "    #other file is for safekeeping of all posts that dont contain keywords. \n",
    "    AIPostFile = f\"{subredditName}_AIposts.json\"\n",
    "    nonAIPostFile = f\"{subredditName}_nonAIPosts.json\"\n",
    "\n",
    "    #parameters for scrape sample\n",
    "    maxAIPosts = 150  \n",
    "    topLimit = 70  #ALL TIME\n",
    "    controversialLimit = 60  #ALL-TIME\n",
    "    hotLimit = 20  #RECENT\n",
    "\n",
    "    #save the post ids of everything already scraped so there arent duplicates\n",
    "    processedPostIDs = set()\n",
    "\n",
    "    #retries for larger loop \n",
    "    maxRetries = 3  \n",
    "    retryCount = 0  \n",
    "\n",
    "    try:\n",
    "        subreddit = reddit.subreddit(subredditName)\n",
    "\n",
    "        totalCollectedAI = countJsonlLines(AIPostFile) #count progress towards 150 AIrelated posts by using the file theyre saved to\n",
    "        \n",
    "        #grab posts from TOP \n",
    "        if totalCollectedAI < maxAIPosts:\n",
    "            logging.info(f\"Fetching {topLimit} posts from TOP...\")\n",
    "            processPosts(subreddit.top(time_filter=\"all\", limit=topLimit), AIPostFile, nonAIPostFile, processedPostIDs)\n",
    "\n",
    "        #grab posts from Controversial\n",
    "        totalCollectedAI = len(loadExistingData(AIPostFile))\n",
    "        if totalCollectedAI < maxAIPosts:\n",
    "            logging.info(f\"Fetching {controversialLimit} posts from CONTROVERSIAL...\")\n",
    "            processPosts(subreddit.controversial(time_filter=\"all\", limit=controversialLimit), AIPostFile, nonAIPostFile, processedPostIDs)\n",
    "\n",
    "        #grab posts from HOT\n",
    "        totalCollectedAI = len(loadExistingData(AIPostFile))\n",
    "        if totalCollectedAI < maxAIPosts:\n",
    "            logging.info(f\"Fetching {hotLimit} posts from HOT...\")\n",
    "            processPosts(subreddit.hot(limit=hotLimit), AIPostFile, nonAIPostFile, processedPostIDs)\n",
    "\n",
    "        #check total collected posts related to AI, then backfill to 150 with posts from TOP\n",
    "        while totalCollectedAI < maxAIPosts:\n",
    "            neededPosts = maxAIPosts - totalCollectedAI\n",
    "            logging.info(f\"Filling gap with {neededPosts} additional TOP posts...\")\n",
    "            processPosts(subreddit.top(time_filter=\"all\", limit=neededPosts), AIPostFile, nonAIPostFile, processedPostIDs)\n",
    "            totalCollectedAI = len(loadExistingData(AIPostFile))\n",
    "\n",
    "            #continue retrying until len 150. \n",
    "            if totalCollectedAI == len(loadExistingData(AIPostFile)):\n",
    "                retryCount += 1  \n",
    "                logging.warning(f\"No new AI posts found. Retry attempt {retryCount}/{maxRetries}.\")\n",
    "            else:\n",
    "                retryCount = 0  \n",
    "            \n",
    "            #in case of small subreddits with few related posts; stops after 3 tries\n",
    "            if retryCount >= maxRetries:\n",
    "                logging.warning(\"Max retries reached. Not enough AI posts in this subreddit. Stopping scrape.\")\n",
    "                break \n",
    "    except (RequestException, ResponseException, ServerError) as e:\n",
    "        logging.error(f\"Reddit API error: {e}. Retrying...\")\n",
    "        handleRateLimit()\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Unexpected error: {e}\")\n",
    "\n",
    "    logging.info(f\"Finished scraping {subredditName}. Total AI posts collected: {totalCollectedAI}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## THIS IS HOW I SORTED POSTS \n",
    "\n",
    "def processPosts(posts, AIPostFile, nonAIPostFile, processedPostIDs, aiKeywords):\n",
    "    \"\"\"\n",
    "    takes in a list of keywords to search for in a post's text content, and uses regex to be sure that word does or does not appear in the post. \n",
    "    searches the comments then sorts by content in comments \n",
    "    \"\"\"\n",
    "    \n",
    "    #regex pattern for finding AI-related keywords. uses word boundaries (\\b) to ensure full word matching\n",
    "    keywordPattern = re.compile(r'\\b(' + '|'.join(re.escape(k) for k in aiKeywords) + r')\\b', re.IGNORECASE)\n",
    "\n",
    "    for post in posts:\n",
    "        try:\n",
    "            if post.id in processedPostIDs:\n",
    "                continue\n",
    "            processedPostIDs.add(post.id)\n",
    "\n",
    "            postText = f\"{post.title} {post.selftext}\".strip().lower()\n",
    "\n",
    "            # Initialize post data\n",
    "            postData = {\n",
    "                \"postId\": post.id,\n",
    "                \"title\": post.title,\n",
    "                \"selftext\": post.selftext,\n",
    "                \"score\": post.score,\n",
    "                \"numComments\": post.num_comments,\n",
    "                \"subreddit\": post.subreddit.display_name,\n",
    "                \"createdUtc\": post.created_utc,\n",
    "                \"url\": post.url,\n",
    "                \"comments\": []\n",
    "            }\n",
    "\n",
    "            # Fetch comments and remove bot-generated ones\n",
    "            post.comments.replace_more(limit=0)\n",
    "            allComments = post.comments.list()\n",
    "\n",
    "            # Separate top and controversial comments\n",
    "            topComments = [c for c in allComments[:25] if not isBotComment(c.body.lower())]\n",
    "            controversialComments = [c for c in allComments[-10:] if not isBotComment(c.body.lower())]\n",
    "\n",
    "            # Filter comments: Must have at least 3 words & score > 1\n",
    "            filteredComments = [\n",
    "                c for c in (topComments + controversialComments)\n",
    "                if len(c.body.split()) >= 3 and c.score > 1\n",
    "            ][:30]  # Keep max 30 comments\n",
    "\n",
    "            for comment in filteredComments:\n",
    "                postData[\"comments\"].append({\n",
    "                    \"commentId\": comment.id,\n",
    "                    \"body\": comment.body,\n",
    "                    \"score\": comment.score,\n",
    "                    \"createdUtc\": comment.created_utc\n",
    "                })\n",
    "\n",
    "            # Skip posts with no text and insufficient comments\n",
    "            if not post.selftext.strip() and len(postData[\"comments\"]) < 5:\n",
    "                logging.info(f\"Skipping post {post.id}: No text and insufficient comments.\")\n",
    "                continue  \n",
    "\n",
    "            #determine post related-ness \n",
    "            # Check if post or comments contain any AI-related keywords (with whole-word matching)\n",
    "            isAiRelated = bool(keywordPattern.search(postText)) or any(\n",
    "                keywordPattern.search(comment[\"body\"]) for comment in postData[\"comments\"]\n",
    "            )\n",
    "\n",
    "            # Save to appropriate file\n",
    "            if isAiRelated:\n",
    "                savePost(postData, AIPostFile)\n",
    "                logging.info(f\"AI-related post saved: {post.id}\")\n",
    "            else:\n",
    "                savePost(postData, nonAIPostFile)\n",
    "                logging.info(f\"Non-AI-related post saved: {post.id}\")\n",
    "            \n",
    "            time.sleep(2)  # Rate-limit API requests\n",
    "    \n",
    "        except (RequestException, ResponseException, ServerError) as e:\n",
    "            logging.error(f\"API Error: {e}. Retrying after delay.\")\n",
    "            time.sleep(60)  # Prevent API block \n",
    "            continue\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Unexpected error processing post {post.id}: {e}\")\n",
    "            continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##CREATE KEYWORDS USING VOCABULARY FROM MAIN SCRAPE OF AI WARS \n",
    "\n",
    "def loadJsonl(filePath):\n",
    "    \"\"\"Loads JSONL data into a list.\"\"\"\n",
    "    data = []\n",
    "    with open(filePath, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "    return data\n",
    "\n",
    "# Preprocessing function\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'@\\w+', '', text)  # Remove usernames\n",
    "    text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Remove punctuation\n",
    "    return text.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AIWars_AIposts.csv\t - AI \t\n",
    "# Art_ArtPosts.csv      - ART \n",
    "# ArtistHate_Aiposts.csv  -AI \t\n",
    "# DigitalPainting_AIDisc.csv    -AI \n",
    "# ART_articles.csv\t\t   -ART\n",
    "# DigitalPainting_ArtPosts.csv -ART\n",
    "# ArtBuisness_AIDisc.csv\t-ART \n",
    "# Illustration_AIDisc.csv   -AI \n",
    "# ArtBusiness_posts.csv\t\t-ART\n",
    "# Illustration_ArtPosts.csv -ART\n",
    "# Art_AIDisc.csv\t\t - AI \t\n",
    "# aiARt_AIposts.csv - AI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "#KEYWORDS = [\"ai art\", \"generative\", \"midjourney\", \"stable diffusion\", \"stealing\", \"originality\", \"automation\"]\n",
    "\n",
    "# keywords = [\n",
    "#     \"AI art\", \"generative art\", \"AI-generated art\", \"Midjourney\", \"Stable Diffusion\", \n",
    "#     \"DALL-E\", \"DALL-E 2\", \"DeepArt\", \"Artbreeder\", \"prompt engineering\", \"diffusion\", \"used AI\", \"using Ai\",\n",
    "#     \"text-to-image\", \"AI ethics\", \"AI copyright\", \"AI plagiarism\", \"AI originality\", \"human artists\", \"made by human\", \"not ai\", \n",
    "#     \"AI stealing\", \"generated\", \"proAi\", \"antiAI\", \" noAI\", \" no Ai \",\n",
    "#     \"software\", \" AI\", \" GAI \", \"GenAI\", \"AIgen\", \"AIart\", \"machine learning\", \"prompt\", \" generative ai\"]\n",
    "\n",
    "aiKeywords = [\n",
    "    \"painting\", \"art\", \"drawing\", \"artist\", \"quality\", \"art style\", \"style\", \"sketches\", \"doodle\", \"artist\", \"AI art\", \"generative art\", \"AI-generated art\", \"Midjourney\", \"Stable Diffusion\", \n",
    "    \"DALL-E\", \"DALL-E 2\", \"DeepArt\", \"Artbreeder\", \"prompt engineering\", \"diffusion\", \"used AI\", \"using Ai\",\n",
    "    \"text-to-image\", \"AI ethics\", \"AI copyright\", \"AI plagiarism\", \"AI originality\", \"human artists\", \"made by human\", \"not ai\", \n",
    "    \"AI stealing\", \"generated\", \"proAi\", \"antiAI\", \" noAI\", \" no Ai \",\n",
    "    \"software\", \" AI\", \" GAI \", \"GenAI\", \"AIgen\", \"AIart\", \"machine learning\", \"prompt\", \" generative ai\"]\n",
    "\n",
    "\n",
    "\n",
    "#didnt feel like making it match exactly so added spaces to two letter keywords \n",
    "botPhrases = [\n",
    "    \"i am a bot\",\n",
    "    \"this is an automated reminder from the mod team\",\n",
    "    \"this action was performed automatically\",\n",
    "    \"please contact the moderators\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##REVISED SCRAPE FUNCTION\n",
    "\n",
    "def scrapeSubreddit(subredditName):\n",
    "    \"\"\"\n",
    "    Scrapes posts from a specified subreddit and classifies them as AI-related or non-AI-related.\n",
    "    \n",
    "    - Creates two JSON files to store AI-related and non-AI-related posts.\n",
    "    - Defines post scrape parameters (limits for different categories).\n",
    "    - Iteratively fetches posts from 'top', 'controversial', and 'hot' categories.\n",
    "    - Ensures no duplicate posts are processed.\n",
    "    - Keeps track of collected AI posts to reach the target limit.\n",
    "    - Implements a retry mechanism to handle cases where not enough AI posts are found.\n",
    "    \"\"\"\n",
    "    logging.info(f\"Starting scrape for subreddit: {subredditName}\")\n",
    "\n",
    "    #create two CSV files, named for the source subreddit, where posts will be filtered.\n",
    "    #one file contains all posts with keywords relating to AI in them\n",
    "    #other file is for safekeeping of all posts that dont contain keywords. \n",
    "    AIPostFile = f\"{subredditName}_AIposts.json\"\n",
    "    nonAIPostFile = f\"{subredditName}_nonAIPosts.json\"\n",
    "\n",
    "    #parameters for scrape sample\n",
    "    maxAIPosts = 150  \n",
    "    topLimit = 70  #ALL TIME\n",
    "    controversialLimit = 60  #ALL-TIME\n",
    "    hotLimit = 20  #RECENT\n",
    "    # ################ TEST ################\n",
    "    # topLimit = 100  #ALL TIME\n",
    "    # controversialLimit = 30  #ALL-TIME\n",
    "    # hotLimit = 20  #RECENT\n",
    "    # #######################################\n",
    "\n",
    "    #save the post ids of everything already scraped so there arent duplicates\n",
    "    processedPostIDs = set()\n",
    "\n",
    "    #retries for larger loop \n",
    "    maxRetries = 3  \n",
    "    retryCount = 0  \n",
    "\n",
    "    try:\n",
    "        subreddit = reddit.subreddit(subredditName)\n",
    "\n",
    "        totalCollectedAI = countJsonlLines(AIPostFile) #count progress towards 150 AIrelated posts by using the file theyre saved to\n",
    "        \n",
    "        #grab posts from TOP \n",
    "        if totalCollectedAI < maxAIPosts:\n",
    "            logging.info(f\"Fetching {topLimit} posts from TOP...\")\n",
    "            processPosts(subreddit.top(time_filter=\"all\", limit=topLimit), AIPostFile, nonAIPostFile, processedPostIDs, aiKeywords)\n",
    "\n",
    "        #grab posts from Controversial\n",
    "        totalCollectedAI = len(loadExistingData(AIPostFile))\n",
    "        if totalCollectedAI < maxAIPosts:\n",
    "            logging.info(f\"Fetching {controversialLimit} posts from CONTROVERSIAL...\")\n",
    "            processPosts(subreddit.controversial(time_filter=\"all\", limit=controversialLimit), AIPostFile, nonAIPostFile, processedPostIDs, aiKeywords)\n",
    "\n",
    "        #grab posts from HOT\n",
    "        totalCollectedAI = len(loadExistingData(AIPostFile))\n",
    "        if totalCollectedAI < maxAIPosts:\n",
    "            logging.info(f\"Fetching {hotLimit} posts from HOT...\")\n",
    "            processPosts(subreddit.hot(limit=hotLimit), AIPostFile, nonAIPostFile, processedPostIDs, aiKeywords)\n",
    "\n",
    "        #check total collected posts related to AI, then backfill to 150 with posts from TOP\n",
    "        while totalCollectedAI < maxAIPosts:\n",
    "            neededPosts = maxAIPosts - totalCollectedAI\n",
    "            logging.info(f\"Filling gap with {neededPosts} additional TOP posts...\")\n",
    "            processPosts(subreddit.top(time_filter=\"all\", limit=neededPosts), AIPostFile, nonAIPostFile, processedPostIDs, aiKeywords)\n",
    "            totalCollectedAI = len(loadExistingData(AIPostFile))\n",
    "\n",
    "            #continue retrying until len 150. \n",
    "            if totalCollectedAI == len(loadExistingData(AIPostFile)):\n",
    "                retryCount += 1  \n",
    "                logging.warning(f\"No new AI posts found. Retry attempt {retryCount}/{maxRetries}.\")\n",
    "            else:\n",
    "                retryCount = 0  \n",
    "            \n",
    "            #in case of small subreddits with few related posts; stops after 3 tries\n",
    "            if retryCount >= maxRetries:\n",
    "                logging.warning(\"Max retries reached. Not enough AI posts in this subreddit. Stopping scrape.\")\n",
    "                break \n",
    "    except (RequestException, ResponseException, ServerError) as e:\n",
    "        logging.error(f\"Reddit API error: {e}. Retrying...\")\n",
    "        handleRateLimit()\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Unexpected error: {e}\")\n",
    "\n",
    "    logging.info(f\"Finished scraping {subredditName}. Total AI posts collected: {totalCollectedAI}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import logging\n",
    "import time\n",
    "from requests.exceptions import RequestException\n",
    "from prawcore.exceptions import ResponseException, ServerError\n",
    "\n",
    "def processPosts(posts, AIPostFile, nonAIPostFile, processedPostIDs, aiKeywords):\n",
    "    \"\"\"\n",
    "    Processes Reddit posts and categorizes them as AI-related or non-AI-related using precise keyword matching.\n",
    "    \n",
    "    - Uses regex to avoid substring issues (e.g., \"paint\" triggering \"AI\").\n",
    "    - Filters bot-generated and low-quality comments.\n",
    "    - Accepts a list of AI-related keywords for flexibility.\n",
    "    \"\"\"\n",
    "    \n",
    "    #regex pattern for finding AI-related keywords. uses word boundaries (\\b) to ensure full word matching\n",
    "    keywordPattern = re.compile(r'\\b(' + '|'.join(re.escape(k) for k in aiKeywords) + r')\\b', re.IGNORECASE)\n",
    "\n",
    "    for post in posts:\n",
    "        try:\n",
    "            if post.id in processedPostIDs:\n",
    "                continue\n",
    "            processedPostIDs.add(post.id)\n",
    "\n",
    "            postText = f\"{post.title} {post.selftext}\".strip().lower()\n",
    "\n",
    "            # Initialize post data\n",
    "            postData = {\n",
    "                \"postId\": post.id,\n",
    "                \"title\": post.title,\n",
    "                \"selftext\": post.selftext,\n",
    "                \"score\": post.score,\n",
    "                \"numComments\": post.num_comments,\n",
    "                \"subreddit\": post.subreddit.display_name,\n",
    "                \"createdUtc\": post.created_utc,\n",
    "                \"url\": post.url,\n",
    "                \"comments\": []\n",
    "            }\n",
    "\n",
    "            # Fetch comments and remove bot-generated ones\n",
    "            post.comments.replace_more(limit=0)\n",
    "            allComments = post.comments.list()\n",
    "\n",
    "            # Separate top and controversial comments\n",
    "            topComments = [c for c in allComments[:25] if not isBotComment(c.body.lower())]\n",
    "            controversialComments = [c for c in allComments[-10:] if not isBotComment(c.body.lower())]\n",
    "\n",
    "            # Filter comments: Must have at least 3 words & score > 3\n",
    "            filteredComments = [\n",
    "                c for c in (topComments + controversialComments)\n",
    "                if len(c.body.split()) >= 3 and c.score > 3\n",
    "            ][:15]  # Keep max 15 comments\n",
    "\n",
    "            for comment in filteredComments:\n",
    "                postData[\"comments\"].append({\n",
    "                    \"commentId\": comment.id,\n",
    "                    \"body\": comment.body,\n",
    "                    \"score\": comment.score,\n",
    "                    \"createdUtc\": comment.created_utc\n",
    "                })\n",
    "\n",
    "            # Skip posts with no text and insufficient comments\n",
    "            if not post.selftext.strip() and len(postData[\"comments\"]) < 5:\n",
    "                logging.info(f\"Skipping post {post.id}: No text and insufficient comments.\")\n",
    "                continue  \n",
    "\n",
    "            #determine post related-ness \n",
    "            # Check if post or comments contain any AI-related keywords (with whole-word matching)\n",
    "            isAiRelated = bool(keywordPattern.search(postText)) or any(\n",
    "                keywordPattern.search(comment[\"body\"]) for comment in postData[\"comments\"]\n",
    "            )\n",
    "            \n",
    "\n",
    "            # Save to appropriate file\n",
    "            if isAiRelated:\n",
    "                savePost(postData, AIPostFile)\n",
    "                logging.info(f\"AI-related post saved: {post.id}\")\n",
    "            else:\n",
    "                savePost(postData, nonAIPostFile)\n",
    "                logging.info(f\"Non-AI-related post saved: {post.id}\")\n",
    "            \n",
    "            time.sleep(2)  # Rate-limit API requests\n",
    "    \n",
    "        except (RequestException, ResponseException, ServerError) as e:\n",
    "            logging.error(f\"API Error: {e}. Retrying after delay.\")\n",
    "            time.sleep(60)  # Prevent API block \n",
    "            continue\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Unexpected error processing post {post.id}: {e}\")\n",
    "            continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrapeSubreddit(\"aiArt\")\n",
    "scrapeSubreddit(\"AIWars\")\n",
    "scrapeSubreddit(\"ArtistHate\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Art-related keywords\n",
    "artKeywords = [ \"painting\", \"art\", \"drawing\", \"artist\", \"quality\", \"art style\", \"style\", \"sketches\", \"doodle\", \"artist\",\"painting\", \"illustration\", \"drawing\", \"fine art\", \"traditional\", \"oil painting\",\n",
    "               \"sculpture\", \"gallery\", \"commission\", \"watercolor\", \"sketching\", \"canvas\", \"pastels\"]\n",
    "\n",
    "\n",
    "aiKeywords = [\n",
    "    \"AI art\", \"generative art\", \"AI-generated art\", \"Midjourney\", \"Stable Diffusion\", \n",
    "    \"DALL-E\", \"DALL-E 2\", \"DeepArt\", \"Artbreeder\", \"prompt engineering\", \"diffusion\", \"used AI\", \"using Ai\",\n",
    "    \"text-to-image\", \"AI ethics\", \"AI copyright\", \"AI plagiarism\", \"AI originality\", \"human artists\", \"made by human\", \"not ai\", \n",
    "    \"AI stealing\", \"generated\", \"proAi\", \"antiAI\", \" noAI\", \" no Ai \", \"AI\", \"artificial intelligence\", \"machine learning\", \"generative\", \"Stable Diffusion\",\n",
    "    \"Midjourney\", \"DALL-E\", \"AI-generated\", \"AI art\", \"prompt\", \"deep learning\", \"GPT\",\n",
    "    \"software\", \" AI\", \" GAI \", \"GenAI\", \"AIgen\", \"AIart\", \"machine learning\", \"prompt\", \" generative ai\"]\n",
    "\n",
    "# regex patterns\n",
    "artPattern = re.compile(r'\\b(' + '|'.join(re.escape(k) for k in artKeywords) + r')\\b', re.IGNORECASE)\n",
    "aiPattern = re.compile(r'\\b(' + '|'.join(re.escape(k) for k in aiKeywords) + r')\\b', re.IGNORECASE)\n",
    "\n",
    "# check top 15 comments for AI discussion\n",
    "def scrapeArtSubreddit(subredditName, limit=100):\n",
    "    logging.info(f\"Starting scrape for Art subreddit: {subredditName}\")\n",
    "\n",
    "    # Output files\n",
    "    artPostsFile = f\"{subredditName}_ArtPosts.json\"\n",
    "    aiDiscussedFile = f\"{subredditName}_AI_Discussions.json\"\n",
    "\n",
    "    #Track processed posts\n",
    "    processedPostIDs = set()\n",
    "\n",
    "    try:\n",
    "        subreddit = reddit.subreddit(subredditName)\n",
    "\n",
    "        posts = list(subreddit.hot(limit=limit)) + \\\n",
    "                list(subreddit.top(time_filter=\"all\", limit=limit)) + \\\n",
    "                list(subreddit.controversial(time_filter=\"all\", limit=limit)) + \\\n",
    "                list(subreddit.new(limit=limit))\n",
    "\n",
    "        collectedAI = 0\n",
    "        for post in posts:\n",
    "            if post.id in processedPostIDs:\n",
    "                continue  # Skip duplicate posts\n",
    "            processedPostIDs.add(post.id)\n",
    "\n",
    "            postText = f\"{post.title} {post.selftext}\".strip().lower()\n",
    "\n",
    "            # Ensure the post is Art-related\n",
    "            if not artPattern.search(postText):\n",
    "                continue  # Skip non-art-related posts\n",
    "\n",
    "            #grab top 15 comments\n",
    "            post.comments.replace_more(limit=0)\n",
    "            allComments = post.comments.list()[:15]\n",
    "\n",
    "            #Fetch comments and remove bot-generated ones\n",
    "            filteredComments = [c.body for c in allComments if not isBotComment(c.body)]\n",
    "\n",
    "            #this is the actual search happening\n",
    "            isAiDiscussed = any(aiPattern.search(comment) for comment in filteredComments) \n",
    "\n",
    "            \n",
    "            postData = {\n",
    "                \"postId\": post.id,\n",
    "                \"title\": post.title,\n",
    "                \"selftext\": post.selftext,\n",
    "                \"subreddit\": subredditName,\n",
    "                \"category\": \"Art\" if not isAiDiscussed else \"AI_Discussion\", #label for if AI is discussed or not \n",
    "                \"num_comments\": post.num_comments,\n",
    "                \"score\": post.score,\n",
    "                \"url\": post.url,\n",
    "                \"top_comments\": filteredComments\n",
    "            }\n",
    "\n",
    "            if isAiDiscussed:\n",
    "                collectedAI += 1\n",
    "                savePost(postData, aiDiscussedFile)\n",
    "                logging.info(f\"AI-related discussion found in comments. Saved post: {post.id}\")\n",
    "            else:\n",
    "                savePost(postData, artPostsFile)\n",
    "                logging.info(f\"Saved non-AI Art post: {post.id}\")\n",
    "\n",
    "        logging.info(f\"Finished scraping {subredditName}. AI-related discussions found in {collectedAI} posts.\")\n",
    "\n",
    "    except (RequestException, ResponseException, ServerError) as e:\n",
    "        logging.error(f\"Reddit API error: {e}. Retrying...\")\n",
    "        handleRateLimit()\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Unexpected error: {e}\")\n",
    "\n",
    "# Function to save posts to JSON\n",
    "def savePost(postData, jsonFile):\n",
    "    with open(jsonFile, 'a') as f:\n",
    "        json.dump(postData, f)\n",
    "        f.write('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean text\n",
    "def clean_text(text, keywords=KEYWORDS_TO_REMOVE):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # Convert to lowercase for consistent processing\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Remove non-ASCII characters (emojis, special symbols)\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', '', text)\n",
    "    \n",
    "    # Remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    \n",
    "    # Remove new lines\n",
    "    text = text.replace(\"\\n\", \" \").replace(\"\\r\", \" \")\n",
    "    \n",
    "    # Remove extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # Remove specific keywords\n",
    "    for keyword in keywords:\n",
    "        text = re.sub(r'\\b' + re.escape(keyword) + r'\\b', '', text, flags=re.IGNORECASE)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def processTextData(df, keywords=KEYWORDS_TO_REMOVE):\n",
    "    # Select relevant text columns\n",
    "    text_columns = [\"title\", \"description\", \"content\"]\n",
    "    \n",
    "    # Apply cleaning function to each text column\n",
    "    for col in text_columns:\n",
    "        df[col] = df[col].apply(lambda x: clean_text(x, keywords))\n",
    "    \n",
    "    # Keep only LABEL, text data columns, and source\n",
    "    cleaned_df = df[[\"LABEL\"] + text_columns + [\"source\"]]\n",
    "    \n",
    "    return cleaned_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new one \n",
    "\n",
    "def cleanText(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = re.sub(r'\\d+', '', text) \n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', '', text) \n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  \n",
    "    return text\n",
    "\n",
    "# Define file mapping for AI and ART categories\n",
    "ai_files = [\n",
    "    \"AIWars_AIposts.csv\", \"ArtistHate_Aiposts.csv\", \"DigitalPainting_AIDisc.csv\",\n",
    "    \"Illustration_AIDisc.csv\", \"Art_AIDisc.csv\", \"aiARt_AIposts.csv\"\n",
    "]\n",
    "\n",
    "art_files = [\n",
    "    \"Art_ArtPosts.csv\", \"DigitalPainting_ArtPosts.csv\",\n",
    "    \"ArtBuisness_AIDisc.csv\", \"ArtBusiness_posts.csv\", \"Illustration_ArtPosts.csv\"\n",
    "]\n",
    "\n",
    "columns = [\"LABEL\", \"source\", \"author\", \"title\", \"description\", \"content\", \"url\", \"publishedAt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define keywords to remove (can be changed outside the function)\n",
    "KEYWORDS_TO_REMOVE = [\"AI art\", \"generative art\", \"AI-generated art\", \"Midjourney\", \"Stable Diffusion\", \n",
    "    \"DALL-E\", \"DALL-E 2\", \"DeepArt\", \"Artbreeder\", \"prompt engineering\", \"diffusion\", \"used AI\", \"using Ai\",\n",
    "    \"text-to-image\", \"AI ethics\", \"AI copyright\", \"AI plagiarism\", \"AI originality\", \"human artists\", \"made by human\", \"not ai\", \n",
    "    \"AI stealing\", \"generated\", \"proAi\", \"antiAI\", \" noAI\", \" no Ai \", \"AI\", \"artificial intelligence\", \"machine learning\", \"generative\", \"Stable Diffusion\",\n",
    "    \"Midjourney\", \"DALL-E\", \"AI-generated\", \"AI art\", \"prompt\", \"deep learning\", \"GPT\",\n",
    "    \"software\", \" AI\", \" GAI \", \"GenAI\", \"AIgen\", \"AIart\", \"machine learning\", \"prompt\", \" generative ai\", \"painting\", \"art\", \"drawing\", \"artist\", \"quality\", \"art style\", \"style\", \"sketches\", \"doodle\", \"artist\",\"painting\", \"illustration\", \"drawing\", \"fine art\", \"traditional\", \"oil painting\",\n",
    "               \"sculpture\", \"gallery\", \"commission\", \"watercolor\", \"sketching\", \"canvas\", \"pastels\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge the csvs as a df\n",
    "\n",
    "def process_csvs(file_list, label):\n",
    "    all_data = []\n",
    "    for file in file_list:\n",
    "        df = pd.read_csv(file, usecols=columns)\n",
    "        all_data.append(df)\n",
    "    \n",
    "    merged_df = pd.concat(all_data, ignore_index=True)\n",
    "    \n",
    "    #save the pre-cleaning version\n",
    "    unclean_filename = f\"uncleaned_text{label}.csv\"\n",
    "    merged_df.to_csv(unclean_filename, quoting=1)\n",
    "    \n",
    "    #run cleaning functions on text columns\n",
    "    for col in [\"title\", \"description\", \"content\"]:\n",
    "        merged_df[col] = merged_df[col].apply(clean_text)\n",
    "    \n",
    "    #save as cleaned\n",
    "    cleaned_filename = f\"{label}_posts.csv\"\n",
    "    merged_df.to_csv(cleaned_filename, quoting=1)\n",
    "    \n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_posts_df = process_csvs(ai_files, \"AI\")\n",
    "art_posts_df = process_csvs(art_files, \"Art\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"uncleaned_textAI.csv\")\n",
    "cleanedDF = processTextData(df)\n",
    "cleanedDF.to_csv(\"cleaned_NO_KW_AIPosts.csv\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LabelDataDF_AI.dropna(subset=[\"content\"], inplace=True)\n",
    "len(LabelDataDF_AI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(LabelDataDF_Art)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LabelDataDF_AI.to_csv(\"LDF-NKW-AiArt.csv\")\n",
    "print (\"Labelled Dataframe: sampled for AIArt.\")\n",
    "LabelDataDF_AI.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LabelDataDF_Art.to_csv(\"LDF-NKW-Art.csv\")\n",
    "print (\"Labelled Dataframe: sampled for ART.\")\n",
    "LabelDataDF_Art.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def removeStopwords(text, stopWords):\n",
    "    tokens = word_tokenize(text)\n",
    "    return \" \".join([word for word in tokens if word.lower() not in stopWords])\n",
    "\n",
    "def lemmatizeText(text, lemmatizer):\n",
    "    tokens = word_tokenize(text)\n",
    "    return \" \".join([lemmatizer.lemmatize(word) for word in tokens])\n",
    "\n",
    "def tokenizeText(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "def vectorizeText(text):\n",
    "    vectorizerCount = CountVectorizer(stop_words='english', max_features=1000)\n",
    "    countMatrix = vectorizerCount.fit_transform(text)\n",
    "    dfCount = pd.DataFrame(countMatrix.toarray(), columns=vectorizerCount.get_feature_names_out())\n",
    "\n",
    "    vectorizerTfidf = TfidfVectorizer(stop_words='english', max_features=1000)\n",
    "    tfidfMatrix = vectorizerTfidf.fit_transform(text)\n",
    "    dfTfidf = pd.DataFrame(tfidfMatrix.toarray(), columns=vectorizerTfidf.get_feature_names_out())\n",
    "    \n",
    "    return dfCount, dfTfidf\n",
    "\n",
    "def processTextData(df, prefix):\n",
    "    stopWords = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    # Ensure \"content\" column is string type\n",
    "    df = df.copy()\n",
    "    df[\"content\"] = df[\"content\"].astype(str)\n",
    "    \n",
    "    # 1. Remove stopwords\n",
    "    dfCleaned = df.copy()\n",
    "    dfCleaned[\"content\"] = dfCleaned[\"content\"].apply(lambda x: removeStopwords(x, stopWords))\n",
    "    \n",
    "    # 2. Lemmatize text\n",
    "    dfLemmatized = dfCleaned.copy()\n",
    "    dfLemmatized[\"content\"] = dfLemmatized[\"content\"].apply(lambda x: lemmatizeText(x, lemmatizer))\n",
    "    \n",
    "    # 3. Tokenize text\n",
    "    dfTokenized = dfCleaned.copy()\n",
    "    dfTokenized[\"content\"] = dfTokenized[\"content\"].apply(tokenizeText)\n",
    "    \n",
    "    # 4. Vectorize the text using CountVectorizer and TfidfVectorizer\n",
    "    dfCount, dfTfidf = vectorizeText(dfCleaned[\"content\"])\n",
    "    \n",
    "    # Print sample outputs\n",
    "    print(f\"Lemmatized DF Sample for {prefix}:\")\n",
    "    print(dfLemmatized.head())\n",
    "    \n",
    "    print(f\"\\nTokenized DF Sample for {prefix}:\")\n",
    "    print(dfTokenized.head())\n",
    "    \n",
    "    print(f\"\\nCountVectorizer DF Sample for {prefix}:\")\n",
    "    print(dfCount.head())\n",
    "    \n",
    "    print(f\"\\nTfidfVectorizer DF Sample for {prefix}:\")\n",
    "    print(dfTfidf.head())\n",
    "    \n",
    "    # Return as DFs with names\n",
    "    return {\n",
    "        f\"{prefix}_lemmatized\": dfLemmatized,\n",
    "        f\"{prefix}_tokenized\": dfTokenized,\n",
    "        f\"{prefix}_count\": dfCount,\n",
    "        f\"{prefix}_tfidf\": dfTfidf\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processTextData(\"LDF-NKW-AiArt.csv\", \"AIArt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processTextData(\"LDF-NKW-Art.csv\", \"Art\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(AIArtDF_tfidf.columns())\n",
    "print(\"AI Art TFIDF\")\n",
    "\n",
    "print(AIArtDF_count.columns())\n",
    "print(\"AIArt Count Vectorized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AIArtDF_count.to_csv(\"AIPosts_CountVectorized.csv\")\n",
    "AIArtDF_lemmatized.to_csv(\"AIPosts_lemmatized.csv\")\n",
    "AIArtDF_tfidf.to_csv(\"AIPosts_tfidf.csv\")\n",
    "AIArtDF_tokenized.to_csv(\"AIPosts_tokenized.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ArtDF_count.to_csv(\"ArtPosts_CountVectorized.csv\")\n",
    "ArtDF_lemmatized.to_csv(\"ArtPosts_lemmatized.csv\")\n",
    "ArtDF_tfidf.to_csv(\"ArtPosts_tfidf.csv\")\n",
    "ArtDF_tokenized.to_csv(\"ArtPosts_tokenized.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_frequencies = ArtDF_count.sum(axis=0).to_dict()\n",
    "\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(word_frequencies)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"WordCloud for ArtDF CountVectorized\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_frequencies = AIArtDF_count.sum(axis=0).to_dict()\n",
    "countVectCloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(word_frequencies)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(countVectCloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"WordCloud for AIArtDF CountVectorized\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ArtDF_count.head(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempDF = AIArtDF_count.drop(columns=[\"like\"], inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_frequencies = tempDF.sum(axis=0).to_dict()\n",
    "countVectCloud2 = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(word_frequencies)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(countVectCloud2, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"WordCloud for AIArtDF CountVectorized\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uncleaned_datasets = {\n",
    "    \"AIARTDF_count.csv\": \"AI\",\n",
    "    \"AIArtDF_tfidf.csv\": \"ART\",\n",
    "    \"AIArtDF_lemmatized.csv\": \"ART\",\n",
    "}\n",
    "\n",
    "cleaned_datasets = {\n",
    "    \"cleaned_NO_KW_AIPosts.csv\": \"AI\",\n",
    "    \"cleaned_NO_KW_ArtPOSTS.csv\": \"ART\",\n",
    "    \"LDF-NKW-AiArt.csv\": \"AI\",\n",
    "    \"LDF-NKW-Art.csv\": \"ART\",\n",
    "}\n",
    "vectorized_datasets = {\n",
    "    \"AIPosts_lemmatized.csv\": \"AI\",\n",
    "    \"AIPosts_tokenized.csv\": \"AI\",\n",
    "    \"ArtPosts_lemmatized.csv\": \"ART\",\n",
    "    \"ArtPosts_tokenized.csv\": \"ART\",\n",
    "}\n",
    "\n",
    "#colors based on sorted categories\n",
    "category_colormaps = {\"AI\": \"Reds\", \"ART\": \"Blues\"}\n",
    "source_colors = {\"NEWS API\": \"lightgray\", \"REDDIT\": \"darkgray\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for wordclouds\n",
    "def generate_wordcloud(filePath, title, category):\n",
    "    df = pd.read_csv(filePath)\n",
    "    text = \" \".join(df.astype(str).values.flatten())\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color=source_colors.get(title.split(\" - \")[0], \"white\"), colormap=category_colormaps.get(category, \"viridis\")).generate(text)\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(title, fontsize=14, fontweight=\"bold\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file, category in cleaned_datasets.items():\n",
    "    filePath = os.path.join(file)\n",
    "    generate_wordcloud(filePath, f\"{file} - [UNCLEANED]\", category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file, category in cleaned_datasets.items():\n",
    "    filePath = os.path.join(file)\n",
    "    generate_wordcloud(filePath, f\"{file} - [UNCLEANED]\", category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for file, category in vectorized_datasets.items():\n",
    " filePath = os.path.join(file)\n",
    "    generate_wordcloud(filePath, f\"{file} - [CLEANED]\", category)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file, category in vectorized_datasets.items():\n",
    "    filePath = os.path.join(os.getcwd(), file)  # Uses current working directory\n",
    "    generate_wordcloud(filePath, f\"{file}\", category)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
