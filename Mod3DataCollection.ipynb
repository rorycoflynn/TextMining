{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89c8dbd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/roryoflynn/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/roryoflynn/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/roryoflynn/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## IMPORTS\n",
    "import pandas as pd\n",
    "import requests\n",
    "import praw\n",
    "import datetime\n",
    "import time\n",
    "import logging\n",
    "import json\n",
    "import re\n",
    "from prawcore.exceptions import RequestException, ResponseException, ServerError\n",
    "from newspaper import Article\n",
    "from newsapi import NewsApiClient\n",
    "from bs4 import BeautifulSoup\n",
    "from requests.exceptions import RequestException\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download necessary NLTK data\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9cbd740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NewsAPI initialization\n",
    "newsKey = '992b1247b5b448e3935baac5af4a841c'  # coryFlynn\n",
    "newsAPI = NewsApiClient(api_key=newsKey)\n",
    "\n",
    "# Example search queries (uncomment to test different patterns)\n",
    "# searchQuery = '\"AI art\" OR \"AI Image\" OR \"AI Artists\" OR \"AI-generated art\" OR \"AI generated images\" OR \"AI generation\" OR \"generative AI art\" OR \"algorithmic art\" -OLED -Pendlebury -Pypi.org -The-next-web -byteDance -samsung'\n",
    "# searchQuery = '\"art\" OR \"painting\" OR \"drawings\" OR \"illustration\" OR \"illustrator\" OR \"graphic design\" OR \"animation\" OR \"fine arts\" OR \"mural\" OR \"creative work\" OR \"art exhibition\" OR \"museum\" OR \"gallery\" -\"AI art\" -\"AI-generated\" -\"AI Images\" -\"machine learning art\" -OLED -The-next-web -byteDance -samsung'\n",
    "\n",
    "# Logging configuration\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,  # see all logs\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    force=True  # rest config\n",
    ")\n",
    "\n",
    "# INIT reddit PRAW API\n",
    "reddit = praw.Reddit(\n",
    "    client_id='JBh-NQdNIpzU5_QWija3LQ',\n",
    "    client_secret='AjBxNAXhL4hPvdWf6xYUpA4sg9ATLg',\n",
    "    user_agent='script:top_posts_scraper:v1.0 (by u/bingabanggg)'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bddbb108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Extracted Full Article ---\n",
      "\n",
      "Project Zomboid's anticipated Build 42 introduced more than gameplay enhancements. The indie game's developer, The Indie Stone, faced backlash after players suspected generative AI was used in the game's new title and loading screen artwork. Before any proof emerged, accusations escalated to harassment, conspiracy theories, and hostility.\n",
      "This controversy highlights the growing debates surrounding AI creativity for artists using generative tools and developers dealing with those opposed to their use. The community's reaction to Build 42 artwork sheds light on broader issues within the anti-AI art movement. It underscores the importance of redirecting efforts toward advocating for regulations on corporations while supporting small independent creators rather than unfairly targeting them.\n",
      "Stop passing off machine learning tricks as real AI improvements\n",
      "Players on Steam and Reddit claimed inconsistencies in the new artwork were telltale signs of generative AI. While the claims weren't unw\n"
     ]
    }
   ],
   "source": [
    "testUrl = \"https://www.androidpolice.com/ai-art-in-project-zomboids-update-sparks-community-outrage/\"\n",
    "\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/97.0.4692.71 Safari/537.36\",\n",
    "    \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "}\n",
    "\n",
    "try:\n",
    "    response = requests.get(testUrl, headers=headers, timeout=10)\n",
    "    response.raise_for_status()  # yell when theres a bad responses\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Extract main article content (i skimmed, and sites use <article> or <p>)\n",
    "    paragraphs = soup.find_all(\"p\")\n",
    "    full_text = \"\\n\".join([p.get_text() for p in paragraphs])\n",
    "\n",
    "    print(\"\\n--- Extracted Full Article ---\\n\")\n",
    "    print(full_text[:1000])  # Print first 1000 characters\n",
    "\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"Error fetching article: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee4f6cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetchNewsArticles(searchQuery, pageSize=20, sortBy=\"relevancy\", testMode=False, fromDate=None):\n",
    "    \"\"\"\n",
    "    request to newsAPI and save as JSON file with all data.\n",
    "    testmode limits the call to one page\n",
    "    \"\"\"\n",
    "    allArticles = []\n",
    "    maxPages = 1 if testMode else 5  # Test mode limits to 1 page for quick testing\n",
    "    if not fromDate:\n",
    "        fromDate = (datetime.datetime.now() - datetime.timedelta(days=30)).strftime('%Y-%m-%d')  # Default to last 30 days\n",
    "\n",
    "    for page in range(1, maxPages + 1):  # get max amnt of pages (API thing)\n",
    "        try:\n",
    "            # tell what page we're on, make api request\n",
    "            print(f\"Fetching page {page}...\")\n",
    "\n",
    "            response = newsAPI.get_everything(\n",
    "                q=searchQuery,\n",
    "                language=\"en\",\n",
    "                sort_by=sortBy,\n",
    "                page=page,\n",
    "                page_size=pageSize,\n",
    "                from_param=fromDate,\n",
    "            )\n",
    "\n",
    "            if \"articles\" in response:  # return count of articles retreived per page\n",
    "                articlesCount = len(response[\"articles\"])\n",
    "                print(f\"Page {page} returned {articlesCount} articles.\")\n",
    "\n",
    "                # Extract relevant fields\n",
    "                for article in response[\"articles\"]:\n",
    "                    # each of these has a value for if it doesnt exist - this helps flagging articles for data cleaning review later\n",
    "                    structured_article = {\n",
    "                        \"source\": article[\"source\"][\"name\"] if article.get(\"source\") else \"Unknown\", # conditional: if source exists, get name, else \"unknown\"\n",
    "                        \"author\": article.get(\"author\", \"Unknown\"),\n",
    "                        \"title\": article.get(\"title\", \"No Title\"),\n",
    "                        \"description\": article.get(\"description\", \"No Description\"),\n",
    "                        \"content\": article.get(\"content\", \"No Content Available\"),\n",
    "                        \"url\": article.get(\"url\", \"No URL\"),\n",
    "                        \"publishedAt\": article.get(\"publishedAt\", \"No Date\"),\n",
    "                    }\n",
    "                    allArticles.append(structured_article)\n",
    "            else:\n",
    "                print(f\"Page {page} returned no articles.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching page {page}: {e}\")\n",
    "            break\n",
    "\n",
    "        # Ratelimit safety\n",
    "        time.sleep(2)\n",
    "\n",
    "    # timestamp of file (for file management craziness)\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    jsonFileName = f\"newsAPI_{timestamp}.json\"\n",
    "\n",
    "    # save and return as JSON\n",
    "    with open(jsonFileName, \"w\", encoding=\"utf-8\") as jsonFile:\n",
    "        json.dump(allArticles, jsonFile, indent=4, ensure_ascii=False)\n",
    "\n",
    "    print(f\"Articles saved to {jsonFileName}\")\n",
    "    return jsonFileName\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d3aee0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadArticlesFromJSON(jsonFile):\n",
    "    \"\"\"\n",
    "    Reads output JSON from fetchNewsArticles articles and converts it to a df\n",
    "    \"\"\"\n",
    "    with open(jsonFile, \"r\", encoding=\"utf-8\") as file:\n",
    "        articles = json.load(file)\n",
    "\n",
    "    df = pd.DataFrame(articles)\n",
    "\n",
    "    # make sure all columns are as they should be (otherwise add them with None values to be flagged later)\n",
    "    expectedColumns = [\"source\", \"author\", \"title\", \"description\", \"content\", \"url\", \"publishedAt\"]\n",
    "    for col in expectedColumns:\n",
    "        if col not in df:\n",
    "            df[col] = None\n",
    "\n",
    "        if col not in df:\n",
    "            df[col] = None\n",
    "\n",
    "    # lambda to each element in source column of DF - checks if element is dict with key 'name' - if yes, return name if not, return unknown.\n",
    "    # if isinstance(source, dict) and \"name\" in source:\n",
    "    #     return source[\"name\"]\n",
    "    # else:\n",
    "    #     return \"Unknown\"\n",
    "    df[\"source\"] = df[\"source\"].apply(lambda x: x[\"name\"] if isinstance(x, dict) and \"name\" in x else \"Unknown\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "687a84a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkExactKeyword(text):\n",
    "    if not isinstance(text, str):\n",
    "        return False\n",
    "    # pad with spaces *as-is* (case preserved)\n",
    "    padded = f\" {text} \"\n",
    "    # tests for lower-case keywords inside THE EXACT padded text\n",
    "    return any(f\" {kw} \" in padded for kw in Keywords)\n",
    "\n",
    "\n",
    "def filterAndProcessArticles(df):\n",
    "    \"\"\"\n",
    "    goes article by article to check for keywords actually IN the article content/body.\n",
    "    then flags for bad-data and has you check them one by one (usually only a dozen will need checking.)\n",
    "    \"\"\"\n",
    "    # Precompile regex pattern for exact keyword matching\n",
    "    keyword_pattern = re.compile(\n",
    "        r'\\b(' + '|'.join(re.escape(k) for k in Keywords) + r')\\b',\n",
    "        flags=re.IGNORECASE\n",
    "    )\n",
    "\n",
    "    # drop any rows missing content\n",
    "    df.dropna(subset=['title', 'description', 'content', 'url'], inplace=True)\n",
    "\n",
    "    # keyword checks on all text columns\n",
    "    df[\"RelevantTitle\"] = df[\"title\"].apply(\n",
    "        lambda text: bool(keyword_pattern.search(text)) if isinstance(text, str) else False\n",
    "    )\n",
    "    df[\"RelevantContent\"] = df[\"content\"].apply(\n",
    "        lambda text: bool(keyword_pattern.search(text)) if isinstance(text, str) else False\n",
    "    )\n",
    "    df[\"RelevantDescription\"] = df[\"description\"].apply(\n",
    "        lambda text: bool(keyword_pattern.search(text)) if isinstance(text, str) else False\n",
    "    )\n",
    "\n",
    "    # articles that dont match any keywords are flagged (new columns)\n",
    "    df[\"KeywordMatch\"] = df[[\"RelevantTitle\", \"RelevantContent\", \"RelevantDescription\"]].any(axis=1)\n",
    "\n",
    "    # all articles from keyword process that got flagged are grabbed for manual review\n",
    "    # all articles that have less than 50 chars are also flagged for review\n",
    "    df[\"ContentLength\"] = df[\"content\"].str.len().fillna(0) + df[\"description\"].str.len().fillna(0)\n",
    "    df[\"NeedsManualReview\"] = (df[\"ContentLength\"] < 50) | (~df[\"KeywordMatch\"])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd46d8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fetchFullArticle(url):\n",
    "    \"\"\"\n",
    "    given url will use newspaper3k first then bs4 to try and get full article text.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        article = Article(url)\n",
    "        article.download()\n",
    "        article.parse()\n",
    "\n",
    "        # check if the len grabbed is significant len\n",
    "        if len(article.text) > 100:\n",
    "            return article.text.strip()\n",
    "\n",
    "        # if newspaper3k falls thru then use bs4\n",
    "        else:\n",
    "            print(f\"Newspaper3k extracted too little content for {url}. Trying BS4 fallback...\")\n",
    "    except Exception as e:\n",
    "        print(f\"Newspaper3k failed for {url}: {e}. Trying BS4 fallback...\")\n",
    "\n",
    "    # using bs4 instead\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "\n",
    "        # skip if blocked\n",
    "        if response.status_code in [403, 429] or \"consent\" in response.url:\n",
    "            print(f\" Skipping (Blocked/Consent Page): {url}\")\n",
    "            return None\n",
    "\n",
    "        response.raise_for_status()\n",
    "\n",
    "        # find p tags and extract text.\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        paragraphs = soup.find_all(\"p\")\n",
    "        fullText = \"\\n\".join([p.get_text() for p in paragraphs]).strip()\n",
    "\n",
    "        return fullText if len(fullText) > 100 else None  # minimum length for results so that bad articles automatically get cut\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"BS4 also failed for {url}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80ee1245",
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveProcessedData(df, cleanFileName, rejectedFileName):\n",
    "    \"\"\"\n",
    "    Takes a DataFrame with a 'NeedsManualReview' column and writes two CSVs:\n",
    "      - approvedRows (NeedsManualReview == False) into cleanFileName\n",
    "      - rejectedRows (NeedsManualReview == True) into rejectedFileName\n",
    "    \"\"\"\n",
    "    approvedRows = df[df[\"NeedsManualReview\"] == False]\n",
    "    rejectedRows = df[df[\"NeedsManualReview\"] == True]\n",
    "\n",
    "    approvedRows.to_csv(cleanFileName, index=False)\n",
    "    rejectedRows.to_csv(rejectedFileName, index=False)\n",
    "\n",
    "    print(f\"Clean articles saved to {cleanFileName}\")\n",
    "    print(f\"Rejected articles saved to {rejectedFileName}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "48869e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#master function to run everything\n",
    "def processAllArticles(jsonFile):\n",
    "    \"\"\"\n",
    "    Master function that loads, processes, and saves articles from a NewsAPI JSON file.\n",
    "    Ensures every article gets full-length content.\n",
    "    \"\"\"\n",
    "    print(\"Loading JSON data...\")\n",
    "    df = loadArticlesFromJSON(jsonFile)\n",
    "\n",
    "    print(\"Filtering and processing articles...\")\n",
    "    df = filterAndProcessArticles(df)\n",
    "\n",
    "    print(\"Fetch full article content for given articles\")\n",
    "    df[\"FullArticleContent\"] = df[\"url\"].apply(fetchFullArticle)\n",
    "\n",
    "    # get output filenames from the JSON input\n",
    "    base = jsonFile.rsplit('.', 1)[0]\n",
    "    cleanFileName = f\"{base}_clean.csv\"\n",
    "    rejectedFileName = f\"{base}_rejected.csv\"\n",
    "\n",
    "    print(\"saving processed data...\")\n",
    "    saveProcessedData(df, cleanFileName, rejectedFileName)\n",
    "\n",
    "    print(\"file complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6463417e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching page 1...\n",
      "Page 1 returned 18 articles.\n",
      "Fetching page 2...\n",
      "Page 2 returned 19 articles.\n",
      "Fetching page 3...\n",
      "Page 3 returned 16 articles.\n",
      "Fetching page 4...\n",
      "Page 4 returned 18 articles.\n",
      "Fetching page 5...\n",
      "Page 5 returned 18 articles.\n",
      "Articles saved to newsAPI_2025-04-26_13-08-27.json\n"
     ]
    }
   ],
   "source": [
    "##FIRST GET ARTICLES ABOUT ART ONLY TO ADD TO THE OLD ARTICLES DATA\n",
    "searchQuery = (\n",
    "    '\"art\" OR \"painting\" OR \"drawings\" OR \"drawing\" OR \"Mural\" OR \"artwork\" OR \"illustration\" OR \"illustrator\" OR \"graphic design\" '\n",
    "    'OR \"animation\" OR \"fine arts\" OR \"mural\" OR \"creative work\" OR \"art exhibition\" '\n",
    "    'OR \"museum\" OR \"gallery\" '\n",
    "    '-\"AI art\" -\"AI-generated\" -\"AI Images\" -\"machine learning art\" '\n",
    "    '-\"OLED\" -\"The-next-web\" -\"byteDance\" -\"samsung\"'\n",
    ")\n",
    "\n",
    "json_file = fetchNewsArticles(\n",
    "    searchQuery=searchQuery,\n",
    "    pageSize=20,# 20 articles per page\n",
    "    sortBy=\"relevancy\",\n",
    "    testMode=False,\n",
    "    fromDate=None,  # None means last 30 days\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "36255735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading JSON data...\n",
      "Filtering and processing articles...\n",
      "Fetch full article content for given articles\n",
      "Newspaper3k failed for https://www.forbes.com/sites/chaddscott/2025/04/15/diego-rivera-frida-kahlo-and-paris-in-two-exhibitions/: Article `download()` failed with 403 Client Error: Max restarts limit reached for url: https://www.forbes.com/sites/chaddscott/2025/04/15/diego-rivera-frida-kahlo-and-paris-in-two-exhibitions/ on URL https://www.forbes.com/sites/chaddscott/2025/04/15/diego-rivera-frida-kahlo-and-paris-in-two-exhibitions/. Trying BS4 fallback...\n",
      "Newspaper3k failed for https://www.forbes.com/sites/robinraven/2025/04/06/msc-cruises-reveals-worlds-biggest-cruise-terminal-and-innovative-art-exhibition/: Article `download()` failed with 403 Client Error: Max restarts limit reached for url: https://www.forbes.com/sites/robinraven/2025/04/06/msc-cruises-reveals-worlds-biggest-cruise-terminal-and-innovative-art-exhibition/ on URL https://www.forbes.com/sites/robinraven/2025/04/06/msc-cruises-reveals-worlds-biggest-cruise-terminal-and-innovative-art-exhibition/. Trying BS4 fallback...\n",
      "Newspaper3k failed for https://www.forbes.com/sites/willmcgough/2025/03/29/an-artist-has-painted-hawaiis-tallest-mural-in-honolulu-heres-where-to-find-it/: Article `download()` failed with 403 Client Error: Max restarts limit reached for url: https://www.forbes.com/sites/willmcgough/2025/03/29/an-artist-has-painted-hawaiis-tallest-mural-in-honolulu-heres-where-to-find-it/ on URL https://www.forbes.com/sites/willmcgough/2025/03/29/an-artist-has-painted-hawaiis-tallest-mural-in-honolulu-heres-where-to-find-it/. Trying BS4 fallback...\n",
      "Newspaper3k failed for https://www.forbes.com/sites/johnkell/2025/04/22/woodford-reserve-blends-art-and-bourbon-for-the-kentucky-derby/: Article `download()` failed with 403 Client Error: Max restarts limit reached for url: https://www.forbes.com/sites/johnkell/2025/04/22/woodford-reserve-blends-art-and-bourbon-for-the-kentucky-derby/ on URL https://www.forbes.com/sites/johnkell/2025/04/22/woodford-reserve-blends-art-and-bourbon-for-the-kentucky-derby/. Trying BS4 fallback...\n",
      "saving processed data...\n",
      "Clean articles saved to newsAPI_2025-04-26_13-08-27_clean.csv\n",
      "Rejected articles saved to newsAPI_2025-04-26_13-08-27_rejected.csv\n",
      "file complete.\n"
     ]
    }
   ],
   "source": [
    "#the words from the query are the keywords now\n",
    "Keywords = [\n",
    "    \"art\", \"painting\", \"drawings\", \"illustration\", \"illustrator\",\n",
    "    \"graphic design\", \"animation\", \"fine arts\", \"mural\", \"creative work\",\n",
    "    \"art exhibition\", \"museum\", \"gallery\"\n",
    "]\n",
    "\n",
    "newArtArticles = processAllArticles('newsAPI_2025-04-26_13-08-27.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "407050b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>content</th>\n",
       "      <th>url</th>\n",
       "      <th>publishedAt</th>\n",
       "      <th>RelevantTitle</th>\n",
       "      <th>RelevantContent</th>\n",
       "      <th>RelevantDescription</th>\n",
       "      <th>KeywordMatch</th>\n",
       "      <th>ContentLength</th>\n",
       "      <th>NeedsManualReview</th>\n",
       "      <th>FullArticleContent</th>\n",
       "      <th>LABEL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Unknown</td>\n",
       "      <td>Debbie Elliott</td>\n",
       "      <td>Rosie the Riveters honored for service in WWII</td>\n",
       "      <td>The National World War Two Museum and the Gary...</td>\n",
       "      <td>NEW ORLEANS A hero's welcome greets 18 women a...</td>\n",
       "      <td>https://www.npr.org/2025/03/30/nx-s1-5332291/r...</td>\n",
       "      <td>2025-03-30T09:00:00Z</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>420</td>\n",
       "      <td>False</td>\n",
       "      <td>Rosie the Riveters honored for service in WWII...</td>\n",
       "      <td>art</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Unknown</td>\n",
       "      <td>info@hypebeast.com (Hypebeast)</td>\n",
       "      <td>Wassily Kandinsky's Sketchbooks Offer New Insi...</td>\n",
       "      <td>Wassily Kandinsky, best known as the pioneer o...</td>\n",
       "      <td>Wassily Kandinsky, best known as the pioneer o...</td>\n",
       "      <td>https://hypebeast.com/2025/4/wassily-kandinsky...</td>\n",
       "      <td>2025-04-07T21:38:38Z</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>473</td>\n",
       "      <td>False</td>\n",
       "      <td>Wassily Kandinsky, best known as the pioneer o...</td>\n",
       "      <td>art</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Unknown</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Rare Tudor paintings show 'fantastical beasts'</td>\n",
       "      <td>The art is found behind plasterwork at a build...</td>\n",
       "      <td>Catherine LeeBBC News, North East and Cumbria\\...</td>\n",
       "      <td>https://www.bbc.com/news/articles/cd6jv5jdp6zo</td>\n",
       "      <td>2025-04-04T06:31:24Z</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>311</td>\n",
       "      <td>False</td>\n",
       "      <td>Rare Tudor paintings show 'fantastical beasts'...</td>\n",
       "      <td>art</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Unknown</td>\n",
       "      <td>Josh Rubin</td>\n",
       "      <td>St. Lucia’s Sugar Beach Is an Art Lover’s Para...</td>\n",
       "      <td>Art meets luxury at a Viceroy Hotels’ newest l...</td>\n",
       "      <td>Art meets luxury at a Viceroy Hotels’ newest l...</td>\n",
       "      <td>https://coolhunting.com/travel/st-lucias-sugar...</td>\n",
       "      <td>2025-04-01T12:49:27Z</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>405</td>\n",
       "      <td>False</td>\n",
       "      <td>Read Travel St. Lucia’s Sugar Beach Is an Art ...</td>\n",
       "      <td>art</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>Unknown</td>\n",
       "      <td>news@appleinsider.com (Malcolm Owen)</td>\n",
       "      <td>Inside Apple Via del Corso -- Rome's store tha...</td>\n",
       "      <td>A visit to the marble-covered Apple Via del Co...</td>\n",
       "      <td>A visit to the marble-covered Apple Via del Co...</td>\n",
       "      <td>https://appleinsider.com/articles/25/04/10/ins...</td>\n",
       "      <td>2025-04-10T16:39:13Z</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>474</td>\n",
       "      <td>False</td>\n",
       "      <td>The outside of Apple Via del Corso in Rome, It...</td>\n",
       "      <td>art</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     source                                author  \\\n",
       "29  Unknown                        Debbie Elliott   \n",
       "22  Unknown        info@hypebeast.com (Hypebeast)   \n",
       "6   Unknown                                   NaN   \n",
       "16  Unknown                            Josh Rubin   \n",
       "60  Unknown  news@appleinsider.com (Malcolm Owen)   \n",
       "\n",
       "                                                title  \\\n",
       "29     Rosie the Riveters honored for service in WWII   \n",
       "22  Wassily Kandinsky's Sketchbooks Offer New Insi...   \n",
       "6      Rare Tudor paintings show 'fantastical beasts'   \n",
       "16  St. Lucia’s Sugar Beach Is an Art Lover’s Para...   \n",
       "60  Inside Apple Via del Corso -- Rome's store tha...   \n",
       "\n",
       "                                          description  \\\n",
       "29  The National World War Two Museum and the Gary...   \n",
       "22  Wassily Kandinsky, best known as the pioneer o...   \n",
       "6   The art is found behind plasterwork at a build...   \n",
       "16  Art meets luxury at a Viceroy Hotels’ newest l...   \n",
       "60  A visit to the marble-covered Apple Via del Co...   \n",
       "\n",
       "                                              content  \\\n",
       "29  NEW ORLEANS A hero's welcome greets 18 women a...   \n",
       "22  Wassily Kandinsky, best known as the pioneer o...   \n",
       "6   Catherine LeeBBC News, North East and Cumbria\\...   \n",
       "16  Art meets luxury at a Viceroy Hotels’ newest l...   \n",
       "60  A visit to the marble-covered Apple Via del Co...   \n",
       "\n",
       "                                                  url           publishedAt  \\\n",
       "29  https://www.npr.org/2025/03/30/nx-s1-5332291/r...  2025-03-30T09:00:00Z   \n",
       "22  https://hypebeast.com/2025/4/wassily-kandinsky...  2025-04-07T21:38:38Z   \n",
       "6      https://www.bbc.com/news/articles/cd6jv5jdp6zo  2025-04-04T06:31:24Z   \n",
       "16  https://coolhunting.com/travel/st-lucias-sugar...  2025-04-01T12:49:27Z   \n",
       "60  https://appleinsider.com/articles/25/04/10/ins...  2025-04-10T16:39:13Z   \n",
       "\n",
       "    RelevantTitle  RelevantContent  RelevantDescription  KeywordMatch  \\\n",
       "29          False             True                 True          True   \n",
       "22           True             True                 True          True   \n",
       "6           False            False                 True          True   \n",
       "16           True             True                 True          True   \n",
       "60           True            False                False          True   \n",
       "\n",
       "    ContentLength  NeedsManualReview  \\\n",
       "29            420              False   \n",
       "22            473              False   \n",
       "6             311              False   \n",
       "16            405              False   \n",
       "60            474              False   \n",
       "\n",
       "                                   FullArticleContent LABEL  \n",
       "29  Rosie the Riveters honored for service in WWII...   art  \n",
       "22  Wassily Kandinsky, best known as the pioneer o...   art  \n",
       "6   Rare Tudor paintings show 'fantastical beasts'...   art  \n",
       "16  Read Travel St. Lucia’s Sugar Beach Is an Art ...   art  \n",
       "60  The outside of Apple Via del Corso in Rome, It...   art  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3 = pd.read_csv('newsAPI_2025-04-26_13-08-27_clean.csv')\n",
    "df3['LABEL'] = 'art'\n",
    "\n",
    "df3.sample(5)  # Print first 5 rows of the DataFrame\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74dd7be",
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [],
   "source": [
    "df3 = df3[['LABEL', 'content', 'FullArticleContent']].copy()\n",
    "df3.rename(columns={\n",
    "    'content': 'newsApiContent',\n",
    "    'FullArticleContent': 'fullArticleContent'\n",
    "}, inplace=True)\n",
    "\n",
    "df3.sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "42e7fcee",
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [],
   "source": [
    "df3.to_csv('VIDEOartnewsAPI.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd618b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#combined ebery single query from the first pass to make the boradest possible thing.\n",
    "\n",
    "searchQuery = (\n",
    "    '\"AI art\" OR \"AI-generated art\" OR \"generative art\" OR \"machine learning art\" '\n",
    "    'OR \"artificial intelligence art\" OR \"algorithmic art\" OR \"Midjourney\" '\n",
    "    'OR \"Stable Diffusion\" OR \"DALL-E\" OR \"Artbreeder\" '\n",
    "    '-\"OLED\" -\"Pendlebury\" -\"Pypi.org\" -\"The-next-web\" -\"byteDance\" -\"samsung\"'\n",
    ")\n",
    "\n",
    "\n",
    "json_file = fetchNewsArticles(\n",
    "    searchQuery=searchQuery,\n",
    "    pageSize=20,# 20 articles per page\n",
    "    sortBy=\"relevancy\",\n",
    "    testMode=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbe5696",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Keywords = [\n",
    "    \"AI art\", \"AI-generated art\", \"generative art\", \"machine learning art\", \"artificial intelligence art\",\n",
    "    \"algorithmic art\",\"Midjourney\", \"Stable Diffusion\", \"DALL-E\", \"Artbreeder\"\n",
    "]\n",
    "\n",
    "newArtArticles = processAllArticles('NewsArticles-AIArt-042225.json')\n",
    "pd.read_csv(\"newsAPI_2025-04-26_13-08-27_clean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bff17f50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['LABEL', 'source', 'author', 'title', 'description', 'content', 'url',\n",
      "       'publishedAt', 'RelevantTitle', 'RelevantContent',\n",
      "       'RelevantDescription', 'KeywordMatch', 'ContentLength',\n",
      "       'NeedsManualReview', 'FullArticleContent'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#add the new data into the old data for combo articles df\n",
    "df_ai  = pd.read_csv('newsAPI_2025-04-26_13-08-27_clean.csv')\n",
    "df_art = pd.read_csv('newsAPI_2025-04-26_13-08-27_clean.csv')\n",
    "\n",
    "df_ai['LABEL']  = 'AIart'\n",
    "df_art['LABEL'] = 'art'\n",
    "\n",
    "df_all = pd.concat([df_ai, df_art], ignore_index=True)\n",
    "\n",
    "cols = ['LABEL'] + [c for c in df_all.columns if c != 'LABEL']\n",
    "df_all = df_all[cols]\n",
    "print(df_all.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe59a6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#these are the original cleaned articles I got from the mod 1 I wanted to add more data to this hence the old code refactoring\n",
    "labeledArtArticles = pd.read_csv('LabeledArtArticles.csv')\n",
    "print(labeledArtArticles.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74dea9c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd393dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mod3 = df_all[['LABEL', 'content', 'FullArticleContent']].copy()\n",
    "df_mod3.rename(columns={\n",
    "    'content': 'newsApiContent',\n",
    "    'FullArticleContent': 'fullArticleContent'\n",
    "}, inplace=True)\n",
    "\n",
    "combined = pd.concat([labeledArtArticles, df_mod3], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7908fe37",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(len(combined))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3eb8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_keywords = [\n",
    "    \"artificial intelligence\", \"machine learning\", \"deep learning\",\n",
    "    \"neural network\", \"generative\", \"algorithmic\", \"gpt\", \"dall[- ]e\",\n",
    "    \"stable diffusion\", \"midjourney\", \"artbreeder\", \"clip\", \"biggan\",\n",
    "    \"bert\", \"transformer\"\n",
    "]\n",
    "art_keywords = [\n",
    "    \"art\", \"painting\", \"drawing\", \"sculpture\", \"illustration\",\n",
    "    \"photograph\", \"photography\", \"gallery\", \"exhibition\",\n",
    "    \"digital art\", \"fine art\", \"graphic design\", \"mixed media\",\n",
    "    \"contemporary art\", \"abstract art\"\n",
    "]\n",
    "\n",
    "\n",
    "#  - matches AI as a whole word (\\bAI\\b)\n",
    "#  - OR AI when immediately followed by a hyphen (\\bAI(?=-))\n",
    "#  - OR any of the other keywords as whole words (\\b(...)\\b)\n",
    "pattern = re.compile(\n",
    "    r'(?:\\bAI\\b|\\bAI(?=-)|\\b(?:' +\n",
    "      '|'.join(map(re.escape, ai_keywords + art_keywords)) +\n",
    "    r')\\b)',\n",
    "    flags=re.IGNORECASE\n",
    ")\n",
    "\n",
    "\n",
    "#any article that mentions any keyword ONCE will be flagged as relevant\n",
    "initial_count = len(df)\n",
    "keep_mask = df.apply(\n",
    "    lambda row: bool(\n",
    "        pattern.search(str(row.get('cleanedContent',''))) or\n",
    "        pattern.search(str(row.get('title',''))) or\n",
    "        pattern.search(str(row.get('description','')))\n",
    "    ),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "\n",
    "removed_count   = initial_count - keep_mask.sum()\n",
    "remaining_count = keep_mask.sum()\n",
    "\n",
    "print(f\"Removed: {removed_count}, Remaining: {remaining_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed5b0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod3NewsDataFINAL = df[keep_mask].copy()\n",
    "mod3NewsDataFINAL.to_csv('mod3NewsDataFINAL.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64958f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956ceb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the words from the query are the keywords now\n",
    "Keywords = [\n",
    "    \"art\", \"painting\", \"drawings\", \"illustration\", \"illustrator\",\n",
    "    \"graphic design\", \"animation\", \"fine arts\", \"mural\", \"creative work\",\n",
    "    \"art exhibition\", \"museum\", \"gallery\"\n",
    "]\n",
    "\n",
    "newArtArticles = processAllArticles('NewsArticles-ART-042225.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef692401",
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_word(s):\n",
    "    return bool(re.search(r'\\w', str(s)))\n",
    "\n",
    "redditAIPosts = pd.read_csv('AIposts-noKW.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b49ff5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "redditArtPosts = pd.read_csv('ArtPosts-noKW.csv')\n",
    "\n",
    "# any reddit post that doesnt have any words in the title, description or content is dropped\n",
    "def has_word(s):\n",
    "    return bool(re.search(r'\\w', str(s)))\n",
    "\n",
    "mask_desc = redditArtPosts['description'].fillna('').astype(str).apply(has_word)\n",
    "mask_cont = redditArtPosts['content'].   fillna('').astype(str).apply(has_word)\n",
    "keep_mask = mask_desc | mask_cont\n",
    "\n",
    "print(f\"Rows before: {len(redditArtPosts)}, removed: {len(redditArtPosts)-keep_mask.sum()}, remaining: {keep_mask.sum()}\")\n",
    "\n",
    "# Filter without losing any columns\n",
    "redditArtPosts1 = redditArtPosts.loc[keep_mask].reset_index(drop=True)\n",
    "\n",
    "redditArtPosts1['content'] = redditArtPosts1['content'].fillna('').astype(str)\n",
    "\n",
    "redditArtPosts1.sample(30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e68fa0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "redditAiPosts = pd.read_csv('AIposts-noKW.csv')\n",
    "\n",
    "#one of the two text columns needs to have text\n",
    "def has_word(s):\n",
    "    return bool(re.search(r'\\w', str(s)))\n",
    "\n",
    "mask_desc = redditAiPosts['description'].fillna('').astype(str).apply(has_word)\n",
    "mask_cont = redditAiPosts['content'].   fillna('').astype(str).apply(has_word)\n",
    "keep_mask = mask_desc | mask_cont\n",
    "\n",
    "print(f\"Rows before: {len(redditAiPosts)}, removed: {len(redditAiPosts)-keep_mask.sum()}, remaining: {keep_mask.sum()}\")\n",
    "\n",
    "redditAiPosts1 = redditAiPosts.loc[keep_mask].reset_index(drop=True)\n",
    "\n",
    "redditAiPosts1['content'] = redditAiPosts1['content'].fillna('').astype(str)\n",
    "\n",
    "redditAiPosts1.sample(30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800e6c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(redditAiPosts1))\n",
    "print(len(redditArtPosts1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96da57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod3RedditDataFINAL = pd.concat([redditAiPosts1, redditArtPosts1], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2bcb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#omg the labels\n",
    "mod3RedditDataFINAL['LABEL'] = mod3RedditDataFINAL['LABEL'].replace({\n",
    "    'ART': 'art',\n",
    "    'Art': 'art',\n",
    "    'AIArt': 'AIart',\n",
    "})\n",
    "\n",
    "print(mod3RedditDataFINAL['LABEL'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a9c9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod3RedditDataFINAL.to_csv('mod3RedditDataFINAL.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd8cfaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "len (redditAiPosts1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8ae70292",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>content</th>\n",
       "      <th>url</th>\n",
       "      <th>publishedAt</th>\n",
       "      <th>RelevantTitle</th>\n",
       "      <th>RelevantContent</th>\n",
       "      <th>RelevantDescription</th>\n",
       "      <th>KeywordMatch</th>\n",
       "      <th>ContentLength</th>\n",
       "      <th>NeedsManualReview</th>\n",
       "      <th>FullArticleContent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Unknown</td>\n",
       "      <td>Elliot Williams</td>\n",
       "      <td>Contagious Ideas</td>\n",
       "      <td>We ran a story about a wall-mounted plotter bo...</td>\n",
       "      <td>We ran a story about a wall-mounted plotter bo...</td>\n",
       "      <td>https://hackaday.com/2025/03/29/contagious-ideas/</td>\n",
       "      <td>2025-03-29T14:00:47Z</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>412</td>\n",
       "      <td>False</td>\n",
       "      <td>We ran a story about a wall-mounted plotter bo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Unknown</td>\n",
       "      <td>Sarah Larson</td>\n",
       "      <td>David Byrne Takes the Stairs</td>\n",
       "      <td>The Talking Heads front man brought his acryli...</td>\n",
       "      <td>At the Pace gallery in Chelsea in early April,...</td>\n",
       "      <td>https://www.newyorker.com/magazine/2025/04/21/...</td>\n",
       "      <td>2025-04-14T10:00:00Z</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>392</td>\n",
       "      <td>False</td>\n",
       "      <td>At the Pace gallery in Chelsea in early April,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Unknown</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The ex-city trader who sold Malala painting fo...</td>\n",
       "      <td>Alexandra Johnson describes how she tackled bo...</td>\n",
       "      <td>Shivani Chaudhari\\r\\nAlexandra Johnson has sol...</td>\n",
       "      <td>https://www.bbc.com/news/articles/cvgn50rqrlgo</td>\n",
       "      <td>2025-04-20T05:35:06Z</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>299</td>\n",
       "      <td>False</td>\n",
       "      <td>The ex-city trader who sold Malala painting fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Unknown</td>\n",
       "      <td>Sabina Graves</td>\n",
       "      <td>Bid on Animation History and Rarities to Raise...</td>\n",
       "      <td>DreamWorks Animation, Sony Pictures Animation,...</td>\n",
       "      <td>The animation industry in partnership with ASI...</td>\n",
       "      <td>https://gizmodo.com/bid-on-animation-history-a...</td>\n",
       "      <td>2025-04-24T23:45:22Z</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>358</td>\n",
       "      <td>False</td>\n",
       "      <td>The animation industry in partnership with ASI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Unknown</td>\n",
       "      <td>Talia Lakritz</td>\n",
       "      <td>See inside Ned's Club, an elite private club i...</td>\n",
       "      <td>A spinoff of Soho House, Ned's Club is a luxe ...</td>\n",
       "      <td>The Gallery at Ned's Club.Frank Frances\\r\\n&lt;ul...</td>\n",
       "      <td>https://www.businessinsider.com/private-member...</td>\n",
       "      <td>2025-04-11T11:47:01Z</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>339</td>\n",
       "      <td>False</td>\n",
       "      <td>This story is available exclusively to Busines...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>Unknown</td>\n",
       "      <td>Benjamin Zhang</td>\n",
       "      <td>I visited a rare Boeing 747 that Delta saved f...</td>\n",
       "      <td>I toured the Boeing 747 Experience at the Delt...</td>\n",
       "      <td>The 747 Experience at the Delta Flight Museum ...</td>\n",
       "      <td>https://www.businessinsider.com/boeing-747-jum...</td>\n",
       "      <td>2025-04-04T15:01:46Z</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>360</td>\n",
       "      <td>False</td>\n",
       "      <td>The 747 Experience at the Delta Flight Museum ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>Unknown</td>\n",
       "      <td>news@appleinsider.com (Malcolm Owen)</td>\n",
       "      <td>Inside Apple Via del Corso -- Rome's store tha...</td>\n",
       "      <td>A visit to the marble-covered Apple Via del Co...</td>\n",
       "      <td>A visit to the marble-covered Apple Via del Co...</td>\n",
       "      <td>https://appleinsider.com/articles/25/04/10/ins...</td>\n",
       "      <td>2025-04-10T16:39:13Z</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>474</td>\n",
       "      <td>False</td>\n",
       "      <td>The outside of Apple Via del Corso in Rome, It...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>Unknown</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Typographic Pictures Composed Entirely of Bras...</td>\n",
       "      <td>The strange art of arranging typographic rule ...</td>\n",
       "      <td>Typographic Portrait of Jean Sibelius Composed...</td>\n",
       "      <td>https://blog.glyphdrawing.club/typographic-pic...</td>\n",
       "      <td>2025-04-13T05:04:01Z</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>289</td>\n",
       "      <td>False</td>\n",
       "      <td>Typographic Portrait of Jean Sibelius Composed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>Unknown</td>\n",
       "      <td>Shannon Carroll</td>\n",
       "      <td>11 of the world’s most expensive and unique ar...</td>\n",
       "      <td>Some of these collections are worth over a bil...</td>\n",
       "      <td>A picture may be worth a thousand words but on...</td>\n",
       "      <td>https://qz.com/most-expensive-unique-art-colle...</td>\n",
       "      <td>2025-04-03T09:00:00Z</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>339</td>\n",
       "      <td>False</td>\n",
       "      <td>A picture may be worth a thousand words — but ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>Unknown</td>\n",
       "      <td>rosie.hilder@futurenet.com (Rosie Hilder)</td>\n",
       "      <td>Tiny art celebrated in new Post-It note drawin...</td>\n",
       "      <td>Collection of detailed drawings launches on Ki...</td>\n",
       "      <td>Acclaimed painter Aron Wiesenfeld has gained f...</td>\n",
       "      <td>https://www.creativebloq.com/art/post-it-note-...</td>\n",
       "      <td>2025-03-28T14:00:20Z</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>276</td>\n",
       "      <td>False</td>\n",
       "      <td>Acclaimed painter Aron Wiesenfeld has gained f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>64 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     source                                     author  \\\n",
       "0   Unknown                            Elliot Williams   \n",
       "1   Unknown                               Sarah Larson   \n",
       "2   Unknown                                        NaN   \n",
       "3   Unknown                              Sabina Graves   \n",
       "4   Unknown                              Talia Lakritz   \n",
       "..      ...                                        ...   \n",
       "59  Unknown                             Benjamin Zhang   \n",
       "60  Unknown       news@appleinsider.com (Malcolm Owen)   \n",
       "61  Unknown                                        NaN   \n",
       "62  Unknown                            Shannon Carroll   \n",
       "63  Unknown  rosie.hilder@futurenet.com (Rosie Hilder)   \n",
       "\n",
       "                                                title  \\\n",
       "0                                    Contagious Ideas   \n",
       "1                        David Byrne Takes the Stairs   \n",
       "2   The ex-city trader who sold Malala painting fo...   \n",
       "3   Bid on Animation History and Rarities to Raise...   \n",
       "4   See inside Ned's Club, an elite private club i...   \n",
       "..                                                ...   \n",
       "59  I visited a rare Boeing 747 that Delta saved f...   \n",
       "60  Inside Apple Via del Corso -- Rome's store tha...   \n",
       "61  Typographic Pictures Composed Entirely of Bras...   \n",
       "62  11 of the world’s most expensive and unique ar...   \n",
       "63  Tiny art celebrated in new Post-It note drawin...   \n",
       "\n",
       "                                          description  \\\n",
       "0   We ran a story about a wall-mounted plotter bo...   \n",
       "1   The Talking Heads front man brought his acryli...   \n",
       "2   Alexandra Johnson describes how she tackled bo...   \n",
       "3   DreamWorks Animation, Sony Pictures Animation,...   \n",
       "4   A spinoff of Soho House, Ned's Club is a luxe ...   \n",
       "..                                                ...   \n",
       "59  I toured the Boeing 747 Experience at the Delt...   \n",
       "60  A visit to the marble-covered Apple Via del Co...   \n",
       "61  The strange art of arranging typographic rule ...   \n",
       "62  Some of these collections are worth over a bil...   \n",
       "63  Collection of detailed drawings launches on Ki...   \n",
       "\n",
       "                                              content  \\\n",
       "0   We ran a story about a wall-mounted plotter bo...   \n",
       "1   At the Pace gallery in Chelsea in early April,...   \n",
       "2   Shivani Chaudhari\\r\\nAlexandra Johnson has sol...   \n",
       "3   The animation industry in partnership with ASI...   \n",
       "4   The Gallery at Ned's Club.Frank Frances\\r\\n<ul...   \n",
       "..                                                ...   \n",
       "59  The 747 Experience at the Delta Flight Museum ...   \n",
       "60  A visit to the marble-covered Apple Via del Co...   \n",
       "61  Typographic Portrait of Jean Sibelius Composed...   \n",
       "62  A picture may be worth a thousand words but on...   \n",
       "63  Acclaimed painter Aron Wiesenfeld has gained f...   \n",
       "\n",
       "                                                  url           publishedAt  \\\n",
       "0   https://hackaday.com/2025/03/29/contagious-ideas/  2025-03-29T14:00:47Z   \n",
       "1   https://www.newyorker.com/magazine/2025/04/21/...  2025-04-14T10:00:00Z   \n",
       "2      https://www.bbc.com/news/articles/cvgn50rqrlgo  2025-04-20T05:35:06Z   \n",
       "3   https://gizmodo.com/bid-on-animation-history-a...  2025-04-24T23:45:22Z   \n",
       "4   https://www.businessinsider.com/private-member...  2025-04-11T11:47:01Z   \n",
       "..                                                ...                   ...   \n",
       "59  https://www.businessinsider.com/boeing-747-jum...  2025-04-04T15:01:46Z   \n",
       "60  https://appleinsider.com/articles/25/04/10/ins...  2025-04-10T16:39:13Z   \n",
       "61  https://blog.glyphdrawing.club/typographic-pic...  2025-04-13T05:04:01Z   \n",
       "62  https://qz.com/most-expensive-unique-art-colle...  2025-04-03T09:00:00Z   \n",
       "63  https://www.creativebloq.com/art/post-it-note-...  2025-03-28T14:00:20Z   \n",
       "\n",
       "    RelevantTitle  RelevantContent  RelevantDescription  KeywordMatch  \\\n",
       "0           False             True                 True          True   \n",
       "1           False             True                 True          True   \n",
       "2            True            False                False          True   \n",
       "3            True             True                 True          True   \n",
       "4           False             True                False          True   \n",
       "..            ...              ...                  ...           ...   \n",
       "59          False             True                 True          True   \n",
       "60           True            False                False          True   \n",
       "61          False             True                 True          True   \n",
       "62           True             True                False          True   \n",
       "63           True            False                 True          True   \n",
       "\n",
       "    ContentLength  NeedsManualReview  \\\n",
       "0             412              False   \n",
       "1             392              False   \n",
       "2             299              False   \n",
       "3             358              False   \n",
       "4             339              False   \n",
       "..            ...                ...   \n",
       "59            360              False   \n",
       "60            474              False   \n",
       "61            289              False   \n",
       "62            339              False   \n",
       "63            276              False   \n",
       "\n",
       "                                   FullArticleContent  \n",
       "0   We ran a story about a wall-mounted plotter bo...  \n",
       "1   At the Pace gallery in Chelsea in early April,...  \n",
       "2   The ex-city trader who sold Malala painting fo...  \n",
       "3   The animation industry in partnership with ASI...  \n",
       "4   This story is available exclusively to Busines...  \n",
       "..                                                ...  \n",
       "59  The 747 Experience at the Delta Flight Museum ...  \n",
       "60  The outside of Apple Via del Corso in Rome, It...  \n",
       "61  Typographic Portrait of Jean Sibelius Composed...  \n",
       "62  A picture may be worth a thousand words — but ...  \n",
       "63  Acclaimed painter Aron Wiesenfeld has gained f...  \n",
       "\n",
       "[64 rows x 14 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(\"newsAPI_2025-04-26_13-08-27_clean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41eb9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## REDDIT HELPER FUNCTIONS\n",
    "def isBotComment(commentText):\n",
    "    botPhrases = [\"i am a bot\", \"automated\", \"bot\", \"auto-mod\"]\n",
    "    return any(phrase in commentText for phrase in botPhrases)\n",
    "\n",
    "def handleRateLimit(waitSeconds=60):\n",
    "    logging.warning(f\"Rate limit reached. Waiting for {waitSeconds} seconds...\")\n",
    "    time.sleep(waitSeconds)\n",
    "\n",
    "def savePost(postData, filePath):\n",
    "    \"\"\"append to jsonl\"\"\"\n",
    "    try:\n",
    "        with open(filePath, 'a', encoding='utf-8') as f:\n",
    "            f.write(json.dumps(postData, ensure_ascii=False) + '\\n')\n",
    "    except IOError as e:\n",
    "        logging.error(f\"Failed to write to file {filePath}: {e}\")\n",
    "\n",
    "def countJsonlLines(filePath):\n",
    "    \"\"\"returns the number of lines in the json file if it exists (filename issues lol)\"\"\"\n",
    "    if os.path.exists(filePath):\n",
    "        with open(filePath, 'r', encoding='utf-8') as f:\n",
    "            return sum(1 for _ in f)\n",
    "    return 0\n",
    "\n",
    "def loadExistingData(filePath):\n",
    "    \"\"\"makes sure json file exists before loading\"\"\"\n",
    "    if not os.path.exists(filePath):\n",
    "        return []\n",
    "    with open(filePath, 'r', encoding='utf-8') as f:\n",
    "        return [json.loads(line) for line in f]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11793621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## commented out bc i dont need it anymore\n",
    "# def processPosts(posts, AIPostFile, nonAIPostFile, processedPostIDs, aiKeywords):\n",
    "#     \"\"\"\n",
    "#     made to clean up the old reddit data i scraped before I cleaned up my process.\n",
    "#     takes in a list of keywords to search for in a post's text content, and uses regex to be sure that word does or does not appear in the post.\n",
    "#     searches the comments then sorts by content in comments\n",
    "#     \"\"\"\n",
    "#     keywordPattern = re.compile(r'\\b(' + '|'.join(re.escape(k) for k in aiKeywords) + r')\\b', re.IGNORECASE)\n",
    "\n",
    "#     for post in posts:\n",
    "#         try:\n",
    "#             if post.id in processedPostIDs:\n",
    "#                 continue\n",
    "#             processedPostIDs.add(post.id)\n",
    "\n",
    "#             postText = f\"{post.title} {post.selftext}\".strip().lower()\n",
    "\n",
    "#             # Initialize post data\n",
    "#             postData = {\n",
    "#                 \"postId\": post.id,\n",
    "#                 \"title\": post.title,\n",
    "#                 \"selftext\": post.selftext,\n",
    "#                 \"score\": post.score,\n",
    "#                 \"numComments\": post.num_comments,\n",
    "#                 \"subreddit\": post.subreddit.display_name,\n",
    "#                 \"createdUtc\": post.created_utc,\n",
    "#                 \"url\": post.url,\n",
    "#                 \"comments\": []\n",
    "#             }\n",
    "\n",
    "#             # Fetch comments and remove bot-generated ones\n",
    "#             post.comments.replace_more(limit=0)\n",
    "#             allComments = post.comments.list()\n",
    "\n",
    "#             # Separate top and controversial comments\n",
    "#             topComments = [c for c in allComments[:25] if not isBotComment(c.body.lower())]\n",
    "#             controversialComments = [c for c in allComments[-10:] if not isBotComment(c.body.lower())]\n",
    "\n",
    "#             # Filter comments: Must have at least 3 words & score > 3\n",
    "#             filteredComments = [\n",
    "#                 c for c in (topComments + controversialComments)\n",
    "#                 if len(c.body.split()) >= 3 and c.score > 3\n",
    "#             ][:15]  # Keep max 15 comments\n",
    "\n",
    "#             for comment in filteredComments:\n",
    "#                 postData[\"comments\"].append({\n",
    "#                     \"commentId\": comment.id,\n",
    "#                     \"body\": comment.body,\n",
    "#                     \"score\": comment.score,\n",
    "#                     \"createdUtc\": comment.created_utc\n",
    "#                 })\n",
    "\n",
    "#             # Skip posts with no text and insufficient comments\n",
    "#             if not post.selftext.strip() and len(postData[\"comments\"]) < 5:\n",
    "#                 logging.info(f\"Skipping post {post.id}: No text and insufficient comments.\")\n",
    "#                 continue\n",
    "\n",
    "#             # determine post related-ness\n",
    "#             isAiRelated = bool(keywordPattern.search(postText)) or any(\n",
    "#                 keywordPattern.search(comment[\"body\"]) for comment in postData[\"comments\"]\n",
    "#             )\n",
    "\n",
    "#             # Save to appropriate file\n",
    "#             if isAiRelated:\n",
    "#                 savePost(postData, AIPostFile)\n",
    "#                 logging.info(f\"AI-related post saved: {post.id}\")\n",
    "#             else:\n",
    "#                 savePost(postData, nonAIPostFile)\n",
    "#                 logging.info(f\"Non-AI-related post saved: {post.id}\")\n",
    "\n",
    "#             time.sleep(2)  # Rate-limit API requests\n",
    "\n",
    "#         except (RequestException, ResponseException, ServerError) as e:\n",
    "#             logging.error(f\"API Error: {e}. Retrying after delay.\")\n",
    "#             time.sleep(60)  # Prevent API block\n",
    "#             continue\n",
    "#         except Exception as e:\n",
    "#             logging.error(f\"Unexpected error processing post {post.id}: {e}\")\n",
    "#             continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1e1638",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrapeSubreddit(subredditName):\n",
    "    \"\"\"\n",
    "    goes through posts and sorts by ai related and nonAI related posts in a given subreddit.\n",
    "    grabs from hot , controverisal, and top and stops when it has 150 total posts related to AI\n",
    "    \"\"\"\n",
    "    logging.info(f\"Starting scrape for subreddit: {subredditName}\")\n",
    "\n",
    "    AIPostFile = f\"{subredditName}_AIposts.json\"\n",
    "    nonAIPostFile = f\"{subredditName}_nonAIPosts.json\"\n",
    "\n",
    "    maxAIPosts = 150\n",
    "    topLimit = 70  # ALL TIME\n",
    "    controversialLimit = 60  # ALL-TIME\n",
    "    hotLimit = 20  # RECENT\n",
    "\n",
    "    processedPostIDs = set()\n",
    "    maxRetries = 3\n",
    "    retryCount = 0\n",
    "\n",
    "    try:\n",
    "        subreddit = reddit.subreddit(subredditName)\n",
    "        totalCollectedAI = countJsonlLines(AIPostFile)\n",
    "\n",
    "        # grab posts from TOP\n",
    "        if totalCollectedAI < maxAIPosts:\n",
    "            logging.info(f\"Fetching {topLimit} posts from TOP...\")\n",
    "            processPosts(subreddit.top(time_filter=\"all\", limit=topLimit),\n",
    "                         AIPostFile, nonAIPostFile, processedPostIDs, aiKeywords)\n",
    "\n",
    "        # grab posts from CONTROVERSIAL\n",
    "        totalCollectedAI = len(loadExistingData(AIPostFile))\n",
    "        if totalCollectedAI < maxAIPosts:\n",
    "            logging.info(f\"Fetching {controversialLimit} posts from CONTROVERSIAL...\")\n",
    "            processPosts(subreddit.controversial(time_filter=\"all\", limit=controversialLimit),\n",
    "                         AIPostFile, nonAIPostFile, processedPostIDs, aiKeywords)\n",
    "\n",
    "        # grab posts from HOT\n",
    "        totalCollectedAI = len(loadExistingData(AIPostFile))\n",
    "        if totalCollectedAI < maxAIPosts:\n",
    "            logging.info(f\"Fetching {hotLimit} posts from HOT...\")\n",
    "            processPosts(subreddit.hot(limit=hotLimit),\n",
    "                         AIPostFile, nonAIPostFile, processedPostIDs, aiKeywords)\n",
    "\n",
    "        # backfill if needed\n",
    "        while totalCollectedAI < maxAIPosts:\n",
    "            neededPosts = maxAIPosts - totalCollectedAI\n",
    "            logging.info(f\"Filling gap with {neededPosts} additional TOP posts...\")\n",
    "            processPosts(subreddit.top(time_filter=\"all\", limit=neededPosts),\n",
    "                         AIPostFile, nonAIPostFile, processedPostIDs, aiKeywords)\n",
    "\n",
    "            newTotal = len(loadExistingData(AIPostFile))\n",
    "            if newTotal == totalCollectedAI:\n",
    "                retryCount += 1\n",
    "                logging.warning(f\"No new AI posts found. Retry attempt {retryCount}/{maxRetries}.\")\n",
    "            else:\n",
    "                totalCollectedAI = newTotal\n",
    "                retryCount = 0\n",
    "\n",
    "            if retryCount >= maxRetries:\n",
    "                logging.warning(\"Max retries reached. Not enough AI posts in this subreddit. Stopping scrape.\")\n",
    "                break\n",
    "\n",
    "    except (RequestException, ResponseException, ServerError) as e:\n",
    "        logging.error(f\"Reddit API error: {e}. Retrying...\")\n",
    "        handleRateLimit()\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Unexpected error: {e}\")\n",
    "\n",
    "    logging.info(f\"Finished scraping {subredditName}. Total AI posts collected: {countJsonlLines(AIPostFile)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c71d431",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TextMining",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
